{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EVALUATIONS_OVER_TEST_SET.ipynb","provenance":[],"collapsed_sections":["LniwSmIxBut7","3iPKAv_BCcHb"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RfekjF9mkdWy"},"source":["# Ajustes iniciales"]},{"cell_type":"markdown","metadata":{"id":"zgdSkhxZdv0_"},"source":["## Conexi√≥n a google drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rEA8wkz2HzN","outputId":"ffea93e8-37a5-4131-d3f4-71cd3ecb4e38"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Crq_ww9d2GNh","outputId":"071a8ed9-edc8-47c5-b865-931dfb0238d8"},"source":["%cd ./drive/MyDrive/Colab Notebooks/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Wm9BK0WAxy_N","outputId":"4160f645-c6a6-41a5-e225-a18c9be6bb45"},"source":["%pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Colab Notebooks'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"8fz70fVKJJ11"},"source":["## Importar modulos *_utils\n"]},{"cell_type":"code","metadata":{"id":"Vm7v_xFpU3sr"},"source":["import sys\n","sys.path.append('/content/drive/My Drive/Colab Notebooks/')\n","\n","from my_utils import eval_utils\n","from my_utils import nn_utils\n","from my_utils import dataset_utils\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tr4J9es5dPjH"},"source":["# Local Utils"]},{"cell_type":"markdown","metadata":{"id":"SUW76L3Nd0DN"},"source":["## Trained Models Evaluations"]},{"cell_type":"markdown","metadata":{"id":"pGKa7AQyQZ0w"},"source":["## evaluateClassifiersOnTest()"]},{"cell_type":"code","metadata":{"id":"s5N-ogC3c4EI"},"source":["import pickle\n","from pandas import DataFrame\n","\n","def evaluateClassifiersOnTest(task, search_results_path, architecture):\n","  ## Load test data\n","  print('Loading test data...')\n","  X_test, Y_test = loadTestData(task, architecture)\n","  print()\n","\n","  ## Load search results\n","  with open(search_results_path, 'rb') as file_handler:\n","    search_results = pickle.load(file_handler)\n","\n","  ## evaluate for method-A pondering (global mean)\n","  print('Evaluating method A models')\n","  search_results.sort_values(by='val_acc_A', ascending=False, inplace=True)\n","  config_ids = search_results.conf_ID[:5].to_list()\n","\n","  merged_results_1 = DataFrame()\n","\n","  for config_id in config_ids[:]:\n","    config_metrics_df = evaluateTrainedModelsOnTestData(X_test, Y_test,\n","                                                       task, \n","                                                       config_id, \n","                                                       'A', \n","                                                       f'{architecture}-1', 7)\n","    merged_results_1=merged_results_1.append(config_metrics_df, ignore_index=True)\n","\n","  results_file = f'./Results/final/{architecture}-1_{task}_TEST.df'\n","\n","  with open(results_file, 'wb') as file_handler:\n","    pickle.dump(merged_results_1, file_handler)\n","\n","  print()\n","\n","  ## evaluate for method-B pondering (per-fold-analysys)\n","  print('Evaluating method B models')\n","\n","  search_results.sort_values(by='val_acc_B', ascending=False, inplace=True)\n","  config_ids = search_results.conf_ID[:5].to_list()\n","\n","  merged_results_2 = DataFrame()\n","\n","  for config_id in config_ids[:]:\n","    config_metrics_df = evaluateTrainedModelsOnTestData(X_test, Y_test,\n","                                                       task, \n","                                                       config_id, \n","                                                       'B', \n","                                                       f'{architecture}-2', 7)\n","    \n","    merged_results_2=merged_results_2.append(config_metrics_df, ignore_index=True)\n","\n","  results_file = f'./Results/final/{architecture}-2_{task}_TEST.df'\n","\n","  with open(results_file, 'wb') as file_handler:\n","    pickle.dump(merged_results_2, file_handler)\n","\n","  print()\n","\n","  return merged_results_1, merged_results_2\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pTOTxhIZMQS3"},"source":["def loadTestData(task, architecture):\n","  if architecture == 'SNN':\n","    encoding_format = 'SINGLE-VEC'\n","  else:\n","    encoding_format = 'EMB-SEQ'\n","\n","  X_test, Y_test = dataset_utils.loadEncodedTestData(embedding_type='FT3',\n","                                                 encoding_format=encoding_format,\n","                                                 labels_to_return = [task])\n","  \n","  return X_test, Y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gmc8sn-id0DP"},"source":["### evaluateTrainedModelsOnTestData()"]},{"cell_type":"code","metadata":{"id":"iVn_WIKAd0DQ"},"source":["from pandas import DataFrame\n","from  tensorflow.keras.utils import to_categorical\n","\n","def evaluateTrainedModelsOnTestData(X_test, Y_test, task, config_ID, eval_method, \n","                                    arch_label, n_folds, verbose=False):\n","\n","  evaluations_record = list()\n","\n","  if task=='HTA':\n","    n_classes = 5\n","  else:\n","    n_classes = 2\n","\n","  # we'll use classes_probs_sum and classes_votes_sum \n","  # to generate two different ensembles \n","  if task=='HTA':\n","    classes_probs_sum = np.zeros((len(Y_test),n_classes))\n","  else:\n","    classes_probs_sum = np.zeros((len(Y_test),1))\n","\n","  classes_votes_sum = np.zeros((len(Y_test),n_classes))\n","\n","  # EVALUATE THE FOLDS CLASSIFIERS ---------------------------------------------\n","  for fold_idx in range(n_folds):\n","    if verbose:\n","      print('\\nEvaluating data-fold {}'.format(fold_idx))\n","  \n","    weights_file = f'F{fold_idx}_{eval_method}.hdf5'\n","    trained_model = loadPretrainedModel(config_ID, weights_file)\n","\n","    # make predictions on X_test samples\n","    classes_probs = trained_model.predict(X_test)\n","    \n","    # turn the prob distributions into classes predictions \n","    labels_predictions_array = getClassesPredictions(classes_probs, task)\n","    \n","    classes_probs_sum += classes_probs\n","    classes_votes_sum += to_categorical(labels_predictions_array, num_classes=n_classes)\n","\n","    # evaluate the preditions\n","    evaluation = evaluatePredictions(task, Y_test[task], labels_predictions_array)\n","    \n","    model_results_dict = {'conf_id': config_ID,\n","                          'model_type': 'F',\n","                          'architecture': arch_label,\n","                        **evaluation}\n","\n","    evaluations_record.append(model_results_dict) \n","\n","  # EVALUATE THE FULL-DATASET MODEL --------------------------------------------\n","  if verbose:\n","    print('\\nEvaluating full-dataset model'.format(fold_idx))\n","\n","  weights_file = f'global_model_{eval_method}.hdf5'\n","  trained_model = loadPretrainedModel(config_ID, weights_file)\n","\n","  # make predictions on X_test samples\n","  classes_probs = trained_model.predict(X_test)\n","  \n","  # turn the prob distributions into classes predictions \n","  labels_predictions_array = getClassesPredictions(classes_probs, task)\n","  \n","  # evaluate the preditions\n","  evaluation = evaluatePredictions(task, Y_test[task], labels_predictions_array)\n","  \n","  model_results_dict = {'conf_id': config_ID,\n","                        'model_type': 'G',\n","                        'architecture': arch_label,\n","                        **evaluation}\n","\n","  evaluations_record.append(model_results_dict) \n","\n","  # EVALUATE THE ENSEMBLES\n","  if verbose:\n","    print('\\nEvaluating MEAN PROBABILITIES ENSEMBLE')\n","\n","  # turn the accumulated probabilities into classes predictions\n","  labels_predictions_array = getClassesPredictions(classes_probs_sum/n_folds, task)\n","  #class_pred_array = np.array([probs.argmax() for probs in classes_probs_sum]).reshape(-1,1)\n","\n","  # evaluate the preditions\n","  evaluation = evaluatePredictions(task, Y_test[task], labels_predictions_array)\n","  \n","  model_results_dict = {'conf_id': config_ID,\n","                        'model_type': 'E1',\n","                        'architecture': arch_label,\n","                        **evaluation}\n","\n","  evaluations_record.append(model_results_dict) \n","\n","  if verbose:\n","    print('\\nEvaluating MAJORITY VOTING ENSEMBLE')\n","\n","  # turn the classes votes into an array of classes predictions\n","  labels_predictions_array = np.array([classes_votes.argmax() for classes_votes in classes_votes_sum]).reshape(-1,1)\n","\n","  # evaluate the preditions\n","  evaluation = evaluatePredictions(task, Y_test[task], labels_predictions_array)\n","  \n","  model_results_dict = {'conf_id': config_ID,\n","                        'model_type': 'E2',\n","                        'architecture': arch_label,\n","                        **evaluation}\n","\n","  evaluations_record.append(model_results_dict) \n","\n","  evaluations_results_df = DataFrame(evaluations_record)\n","\n","  print('*', end=\"\")\n","\n","  return evaluations_results_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJYO5vQQd0DR"},"source":["### loadPretrainedModel()"]},{"cell_type":"code","metadata":{"id":"qZbtm8kCd0DS"},"source":["#*************************     loadPretrainedModel()    ************************  \n","from keras.models import model_from_json\n","\n","def loadPretrainedModel(config_ID, weights_file):\n","  # load model configuration from json file\n","  json_file = open(f'./models_json_files/{config_ID}.json', 'r')\n","  model_config = json_file.read()\n","  json_file.close()\n","  trained_model = model_from_json(model_config)\n","\n","  # load pretrained weights into the model\n","  trained_model.load_weights(f'./trained_models/{config_ID}/{weights_file}')\n","\n","  return trained_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jxk7mtkOd0DT"},"source":["### getClassesPredictions()"]},{"cell_type":"code","metadata":{"id":"R4iNGiQdd0Dc"},"source":["import numpy as np\n","\n","def class_pred(true_prob):\n","  if true_prob>=0.5:\n","    return 1\n","  else:\n","    return 0\n","\n","def getClassesPredictions(classes_probs, task):\n","  if task=='HTA': \n","    return np.array([probs.argmax() for probs in classes_probs]).reshape(-1,1)\n","  else:\n","    return np.apply_along_axis(class_pred, 1, classes_probs).reshape(-1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DkMGjcm8d0Dd"},"source":["### evaluatePredictions()"]},{"cell_type":"code","metadata":{"id":"nvqpCQ4Ud0Df"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","def evaluatePredictions(task, val_labels, pred_labels, verbose=False):\n","\n","  if task=='HTA':\n","\n","    # get the correspondig predicted and val (target) labels for each task\n","    pred_HS_labels, pred_TR_labels, pred_AG_labels = dataset_utils.getLabelsPerTask(pred_labels)\n","    val_HS_labels, val_TR_labels, val_AG_labels = dataset_utils.getLabelsPerTask(val_labels)\n","\n","    # compute the different metrics\n","    HS_acc = accuracy_score(val_HS_labels, pred_HS_labels)\n","    HS_prec = precision_score(val_HS_labels, val_HS_labels, average=\"macro\")\n","    HS_recall = recall_score(val_HS_labels, val_HS_labels, average=\"macro\")\n","    HS_f1 = f1_score(val_HS_labels, pred_HS_labels, average=\"macro\")\n","\n","    AG_acc = accuracy_score(val_AG_labels, pred_AG_labels)\n","    AG_prec = precision_score(val_AG_labels, pred_AG_labels, average=\"macro\")\n","    AG_recall = recall_score(val_AG_labels, pred_AG_labels, average=\"macro\")\n","    AG_f1 = f1_score(val_AG_labels, pred_AG_labels, average=\"macro\")\n","\n","    TR_acc = accuracy_score(val_TR_labels, pred_TR_labels)\n","    TR_prec = precision_score(val_TR_labels, pred_TR_labels, average=\"macro\")\n","    TR_recall = recall_score(val_TR_labels, pred_TR_labels, average=\"macro\")\n","    TR_f1 = f1_score(val_TR_labels, pred_TR_labels, average=\"macro\")\n","\n","    F1_multi = (HS_f1+ AG_f1 + TR_f1)/3\n","\n","    EMR = computeEMR(list(zip(val_HS_labels, val_TR_labels, val_AG_labels)),\n","                    list(zip(pred_HS_labels, pred_TR_labels, pred_AG_labels)))\n","\n","    results_dict = {'HS_acc':HS_acc,\n","                    'HS_prec':HS_prec,\n","                    'HS_recall':HS_recall,\n","                    'HS_f1':HS_f1,\n","                    'AG_acc':AG_acc,\n","                    'AG_prec':AG_prec,\n","                    'AG_recall':AG_recall,\n","                    'AG_f1':AG_f1,\n","                    'TR_acc':TR_acc,\n","                    'TR_prec':TR_prec,\n","                    'TR_recall':TR_recall,\n","                    'TR_f1':TR_f1,\n","                    'F1_multi':F1_multi,\n","                    'EMR':EMR}\n","\n","    if verbose:\n","      print('EMR = ', EMR)\n","      print('F1_multi = ', F1_multi)\n","      print()\n","\n","    return results_dict\n","\n","  if task in ['HS', 'TR', 'AG']:\n","    # compute the different metrics\n","    acc = accuracy_score(val_labels, pred_labels)\n","    prec = precision_score(val_labels, pred_labels, average=\"macro\")\n","    recall = recall_score(val_labels, pred_labels, average=\"macro\")\n","    f1_macro = f1_score(val_labels, pred_labels, average=\"macro\")\n","\n","    results_dict = {'acc':acc,\n","                    'prec':prec,\n","                    'recall':recall,\n","                    'f1-macro':f1_macro}\n","\n","    if verbose:\n","      print('Acc = ', acc)\n","      print('F1_macro = ', f1_macro)\n","      print()\n","\n","    return results_dict\n","\n","def computeEMR(test_labels, pred_labels):\n","  total_instances = len(test_labels)\n","  exact_match_count= 0\n","  for gold, pred in zip(test_labels, pred_labels):\n","    #print(gold, pred)\n","    if gold == pred:\n","      exact_match_count += 1\n","\n","  return exact_match_count/total_instances\n","\n","def compute_metrics(target, predicted):\n","    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","    accuracy = accuracy_score(target, predicted)\n","    precision = precision_score(target, predicted, average=\"macro\")\n","    recall = recall_score(target, predicted, average=\"macro\")\n","    f1 = f1_score(val_labels, pred_labels, average=\"macro\")\n","\n","    results = {'acc':accuracy_s, \n","              'prec' : precision_pos,\n","              'recall' : precision_neg,  \n","              'f1': recall_pos,\n","              'recall_neg' : recall_neg,\n","              'f1_pos': f1_pos,\n","              'f1_neg': f1_neg}\n","    \n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"87-lC8pyNpDs"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","def evaluatePredictions(task, val_labels, pred_labels, verbose=False):\n","\n","  if task=='HTA':\n","\n","    # get the correspondig predicted and val (target) labels for each task\n","    pred_HS_labels, pred_TR_labels, pred_AG_labels = dataset_utils.getLabelsPerTask(pred_labels)\n","    val_HS_labels, val_TR_labels, val_AG_labels = dataset_utils.getLabelsPerTask(val_labels)\n","\n","    # compute ACC, PREC, RECALL and F1 metrics\n","    HS_acc, HS_prec, HS_recall, HS_f1 = compute_metrics(val_HS_labels, pred_HS_labels)\n","    AG_acc, AG_prec, AG_recall, AG_f1 = compute_metrics(val_AG_labels, pred_AG_labels)   \n","    TR_acc, TR_prec, TR_recall, TR_f1 = compute_metrics(val_TR_labels, pred_TR_labels)\n","\n","    F1_multi = (HS_f1+ AG_f1 + TR_f1)/3\n","\n","    EMR = computeEMR(list(zip(val_HS_labels, val_TR_labels, val_AG_labels)),\n","                    list(zip(pred_HS_labels, pred_TR_labels, pred_AG_labels)))\n","\n","    results_dict = {'HS_acc':HS_acc,\n","                    'HS_prec':HS_prec,\n","                    'HS_recall':HS_recall,\n","                    'HS_f1':HS_f1,\n","                    'AG_acc':AG_acc,\n","                    'AG_prec':AG_prec,\n","                    'AG_recall':AG_recall,\n","                    'AG_f1':AG_f1,\n","                    'TR_acc':TR_acc,\n","                    'TR_prec':TR_prec,\n","                    'TR_recall':TR_recall,\n","                    'TR_f1':TR_f1,\n","                    'F1_multi':F1_multi,\n","                    'EMR':EMR}\n","\n","    if verbose:\n","      print('EMR = ', EMR)\n","      print('F1_multi = ', F1_multi)\n","      print()\n","\n","    return results_dict\n","\n","  if task in ['HS', 'TR', 'AG']:\n","    # compute ACC, PREC, RECALL and F1 metrics\n","    acc, prec, recall, f1_macro = compute_metrics(val_labels, pred_labels)\n","\n","    results_dict = {'acc':acc,\n","                    'prec':prec,\n","                    'recall':recall,\n","                    'f1-macro':f1_macro}\n","\n","    if verbose:\n","      print('Acc = ', acc)\n","      print('F1_macro = ', f1_macro)\n","      print()\n","\n","    return results_dict\n","\n","def compute_metrics(target, predicted):\n","    accuracy = accuracy_score(target, predicted)\n","    precision = precision_score(target, predicted, average=\"macro\")\n","    recall = recall_score(target, predicted, average=\"macro\")\n","    f1 = f1_score(target, predicted, average=\"macro\")\n","    \n","    return accuracy, precision, recall, f1    \n","\n","def computeEMR(test_labels, pred_labels):\n","  total_instances = len(test_labels)\n","  exact_match_count= 0\n","  for gold, pred in zip(test_labels, pred_labels):\n","    #print(gold, pred)\n","    if gold == pred:\n","      exact_match_count += 1\n","\n","  return exact_match_count/total_instances"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uer7Lz3id0Dg"},"source":["### labels_utils"]},{"cell_type":"code","metadata":{"id":"E-t-4QHWd0Dg"},"source":["#**************************     getLabelsPerTask()    **************************\n","def getLabelsPerTask(HTA_labels):\n","\n","    HS_labels = list()\n","    TR_labels = list()\n","    AG_labels = list()\n","\n","    for HTA_label in HTA_labels:\n","        HS_label, TR_label, AG_label = mapTo3DimsFormat(HTA_label)\n","\n","        HS_labels.append(HS_label)\n","        TR_labels.append(TR_label)\n","        AG_labels.append(AG_label)\n","\n","    HS_labels = np.array(HS_labels).reshape(-1,1)\n","    TR_labels = np.array(TR_labels).reshape(-1,1)\n","    AG_labels = np.array(AG_labels).reshape(-1,1)\n","\n","    return (HS_labels, TR_labels, AG_labels)\n","\n","#**************************     mapTo3DimsFormat()    ************************** \n","def mapTo3DimsFormat(AB_label):\n","  '''\n","  Maps label in five_classes_format to 3 dims labeling.\n","\n","    0 -> (0,0,0)  [HT = 0, TR = 0, AG = 0]\n","    1 -> (1,0,0)  [HT = 1, TR = 0, AG = 0]\n","    2 -> (1,0,1)  [HT = 1, TR = 0, AG = 1]\n","    3 -> (1,1,0)  [HT = 1, TR = 1, AG = 0]\n","    4 -> (1,1,1)  [HT = 1, TR = 1, AG = 1]\n","\n","  inpunt:\n","  label    - int, label in five_classes_format\n","\n","  output:\n","  (H,T,A)  - ints tuple, labeling in 3 dims format\n","\n","  '''\n","  if AB_label == 0:\n","    return(0,0,0)\n","\n","  elif AB_label == 1:\n","    return(1,0,0)\n","\n","  elif AB_label == 2:\n","    return(1,0,1)\n","\n","  elif AB_label == 3:\n","    return(1,1,0)\n","\n","  elif AB_label == 4:\n","    return(1,1,1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B3yPx4uAcrCg"},"source":["# SNN (COMPLETE)"]},{"cell_type":"markdown","metadata":{"id":"Inuz50o7RAs-"},"source":["## HS"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDre7Vb3RDmC","outputId":"d447a82b-fff9-44c0-d16d-7ffd69995f44"},"source":["results_A, results_B = evaluateClassifiersOnTest('HS', \n","                          search_results_path = './Results/SNN/HS/experiments_4.df',\n","                          architecture = 'SNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: SINGLE-VEC\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (300,)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OVJ9nD3WYvxr"},"source":["## AG"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"soA50h1CYvyJ","outputId":"d8bd9b9c-dac6-4297-a486-3c42241217ad"},"source":["results_A, results_B = evaluateClassifiersOnTest('AG', \n","                          search_results_path = './Results/SNN/AG/experiments_4.df',\n","                          architecture = 'SNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: SINGLE-VEC\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (300,)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1gaI4GjLUrH1"},"source":["## TR"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86skok8FUrH5","outputId":"0020408f-bab8-4d5b-d77e-8338ce371adb"},"source":["results_A, results_B = evaluateClassifiersOnTest('TR', \n","                          search_results_path = './Results/SNN/TR/experiments_4.df',\n","                          architecture = 'SNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: SINGLE-VEC\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (300,)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0xUlwHCyaSX5"},"source":["## HTA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ww1BFc8laSX8","outputId":"2737e503-4f53-4fd3-877e-437dcfe30106"},"source":["results_A, results_B = evaluateClassifiersOnTest('HTA', \n","                          search_results_path = './Results/SNN/HTA/experiments_4f.df',\n","                          architecture = 'SNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: SINGLE-VEC\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (300,)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vm1IhlVXijTY"},"source":["# CNN (COMPLETE)"]},{"cell_type":"markdown","metadata":{"id":"7ko05XgFijTn"},"source":["## HS"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2vJfdV9SijTp","outputId":"5a1e1795-d6d5-41c8-d504-58e0a070f9c4"},"source":["results_A, results_B = evaluateClassifiersOnTest('HS', \n","                          search_results_path = './Results/CNN/HS/experiments_7f.df',\n","                          architecture = 'CNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yaLm7VKiXPvP"},"source":["## AG"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0J-gKp_HXPvd","outputId":"6e9e8fae-e674-432f-cd52-99db2047eca1"},"source":["results_A, results_B = evaluateClassifiersOnTest('AG', \n","                          search_results_path = './Results/CNN/AG/experiments_6.df',\n","                          architecture = 'CNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KGuxvzMuijTz"},"source":["## TR"]},{"cell_type":"code","metadata":{"id":"CzUSjBvkijT0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"493324ee-eb80-4835-bbfe-fe0f8f6ade26"},"source":["results_A, results_B = evaluateClassifiersOnTest('TR', \n","                          search_results_path = './Results/CNN/TR/experiments_6f.df',\n","                          architecture = 'CNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yGuNxriyXPvh"},"source":["## HTA"]},{"cell_type":"code","metadata":{"id":"ls0Ez8KVXPvi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8beab19d-4fab-4bdc-e600-8b1c4b89e207"},"source":["results_A, results_B = evaluateClassifiersOnTest('HTA', \n","                          search_results_path = './Results/CNN/HTA/experiments_6f.df',\n","                          architecture = 'CNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K4i_v9vhQgto"},"source":["# BiLSTM (COMPLETE)"]},{"cell_type":"markdown","metadata":{"id":"D94DPmc6Qgtp"},"source":["## HS"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OEVP9I_Qgtq","outputId":"f017e229-1abe-45ce-870c-2e5bc1a4143f"},"source":["results_A, results_B = evaluateClassifiersOnTest('HS', \n","                          search_results_path = './Results/BiLSTM/HS/experiments_4f.df',\n","                          architecture = 'BiLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_sZ_MMKAQgtu"},"source":["## AG"]},{"cell_type":"code","metadata":{"id":"H_D1TPNCQgtu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1f7d3983-ce5c-410a-e626-977b7c32ccd0"},"source":["results_A, results_B = evaluateClassifiersOnTest('AG', \n","                          search_results_path = './Results/BiLSTM/AG/experiments_4f.df',\n","                          architecture = 'BiLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ESdC3FCLQgts"},"source":["## TR"]},{"cell_type":"code","metadata":{"id":"vi4HJeJZQgts","colab":{"base_uri":"https://localhost:8080/"},"outputId":"91cd0d17-bd81-47df-9183-989602818088"},"source":["results_A, results_B = evaluateClassifiersOnTest('TR', \n","                          search_results_path = './Results/BiLSTM/TR/experiments_4f.df',\n","                          architecture = 'BiLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HbicdY29Qgtw"},"source":["## HTA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y4JG_1qUQgtw","outputId":"2046a261-8f89-440f-c7ab-fd3be602be46"},"source":["results_A, results_B = evaluateClassifiersOnTest('HTA', \n","                          search_results_path = './Results/BiLSTM/HTA/experiments_4f.df',\n","                          architecture = 'BiLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LUJDQHg4QkeD"},"source":["# ConvLSTM"]},{"cell_type":"markdown","metadata":{"id":"kkGT4zjRQkeE"},"source":["## HS"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vqpCPzIPQkeF","outputId":"6babb74e-b034-4364-f595-ec240ce87dad"},"source":["results_A, results_B = evaluateClassifiersOnTest('HS', \n","                          search_results_path = './Results/ConvLSTM/HS/experiments_5f.df',\n","                          architecture = 'ConvLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2wrW0nbwQkeL"},"source":["## AG"]},{"cell_type":"code","metadata":{"id":"YKFpE98WQkeQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"79b4d2fc-c9ba-4c04-d524-cb97add0a133"},"source":["results_A, results_B = evaluateClassifiersOnTest('AG', \n","                          search_results_path = './Results/ConvLSTM/AG/experiments_5f.df',\n","                          architecture = 'ConvLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qxjaaxQPQkeI"},"source":["## TR"]},{"cell_type":"code","metadata":{"id":"-YEEq8cGQkeJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e0169ea4-0f13-415d-e495-0c93e3438dfe"},"source":["results_A, results_B = evaluateClassifiersOnTest('TR', \n","                          search_results_path = './Results/ConvLSTM/TR/experiments_5f.df',\n","                          architecture = 'ConvLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"im2noR1TQkeS"},"source":["## HTA"]},{"cell_type":"code","metadata":{"id":"ZlOhLPxPQkeT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"599f56b6-aeff-4429-e37e-570993015245"},"source":["results_A, results_B = evaluateClassifiersOnTest('HTA', \n","                          search_results_path = './Results/ConvLSTM/HTA/experiments_5f.df',\n","                          architecture = 'ConvLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Evaluating method B models\n","*****\n"],"name":"stdout"}]}]}