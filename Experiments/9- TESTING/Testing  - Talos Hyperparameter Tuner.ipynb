{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Testing  - Talos Hyperparameter Tuner.ipynb","provenance":[{"file_id":"1GsS09QS9FfkZRsK6y-b5yZ9S68ZJ2vvV","timestamp":1607846483277},{"file_id":"103D0flkS-Uip7ez0rElakUgwKa-KsT-j","timestamp":1606335668638}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1GsS09QS9FfkZRsK6y-b5yZ9S68ZJ2vvV","authorship_tag":"ABX9TyMBxZhTBIRJnQIQBk7oYLBZ"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"q5EmzImyiRQR"},"source":["# Setting Up"]},{"cell_type":"markdown","metadata":{"id":"UwqNcg7jbu2l"},"source":["### Move to base folder"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W9X3nCv_NaxP","executionInfo":{"status":"ok","timestamp":1607679267315,"user_tz":360,"elapsed":643,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"cb15ea1d-e6a6-4c3a-dbbf-cc5adf8a5676"},"source":["%cd ./drive/MyDrive/Colab Notebooks"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N41V_OgRDo9J"},"source":["### Installing Talos"]},{"cell_type":"code","metadata":{"id":"iVsLT34F4RwT"},"source":["% pip install talos"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uJi6IoaKU2mD"},"source":["# Utils"]},{"cell_type":"markdown","metadata":{"id":"NVs71GWw6wAo"},"source":["## compute_metrics()"]},{"cell_type":"markdown","metadata":{"id":"7bujXKeB6p2x"},"source":["## load_encoded_corpus()"]},{"cell_type":"code","metadata":{"id":"S1vPgV1IU2mH"},"source":["## LOAD ENCODED CORPUS\n","def load_encoded_corpus(encoding, scaling = None):\n","  \"\"\" \n","  Loads encoded dataset from drive. Returns train and test sets ready to be \n","  fed into training algorithm.\n","  \n","  Input:\n","  encoding_id  -- str\n","\n","  Output:\n","  (X_train, y_train, X_test, y_test) -- tuple containing the encoded dataset\n","\n","  \"\"\"\n","  import pickle\n","  import numpy as np\n","\n","  encodings_info = {'FT1':'FastText 1 - Common Crawl + Wikipedia',\n","                    'FT2':'FastText 2 - Esp. Wikipedia',\n","                    'FT3':'FastText 3 - Spanish Unannotated Corpora',\n","                    'W2V1':'W2V 1 - Spanish Unannotated Corpora',\n","                    'W2V2':'W2V 2 - Spanish CoNLL',\n","                    'GloVe_300d':'GloVe 300d - Spanish Billion Word Corpus',\n","                    'GloVe_100d':'GloVe 100d - Spanish Billion Word Corpus'\n","                    }\n","  \n","  paths_dict = {'FT1': ('dataset_files/EncodedTrainTweets_FastText1_pr2', \n","                        'dataset_files/EncodedTestTweets_FastText1_pr2'),\n","                \n","                'FT2': ('dataset_files/EncodedTrainTweets_FastText2_pr2',\n","                        'dataset_files/EncodedTestTweets_FastText2_pr2'),\n","                \n","                'FT3': ('dataset_files/EncodedTrainTweets_FastText3_pr2',\n","                        'dataset_files/EncodedTestTweets_FastText3_pr2'),\n","                \n","                'W2V1': ('dataset_files/EncodedTrainTweets_W2V2_pr2',\n","                         'dataset_files/EncodedTestTweets_W2V2_pr2'),\n","                \n","                'W2V2': ('dataset_files/EncodedTrainTweets_GloVe300d_pr2',\n","                         'dataset_files/EncodedTestTweets_GloVe300d_pr2'),\n","                \n","                'GloVe_300d': ('dataset_files/EncodedTrainTweets_GloVe300d_pr2',\n","                               'dataset_files/EncodedTestTweets_GloVe300d_pr2'),\n","                \n","                'GloVe_100d': ('dataset_files/EncodedTrainTweets_GloVe100d_pr2',\n","                               'dataset_files/EncodedTestTweets_GloVe100d_pr2')\n","                }\n","\n","  if encoding in encodings_info.keys():\n","    encoding_info = encodings_info[encoding]\n","    train_file_path,test_file_path = paths_dict[encoding]\n","  else:\n","    print('Unknown encoding. Function has been terminated.')\n","    return\n","\n","  print(encoding_info)\n","  print('Encoded tweets and labels are being loaded.')\n","\n","  ## Retrieve encoded Tweets\n","  with open(train_file_path, 'rb') as filehandle:\n","      # store the encoded documents as binary data\n","      X_train = pickle.load(filehandle)\n","\n","  with open(test_file_path, 'rb') as filehandle:\n","      # store the encoded documents as binary data\n","      X_test = pickle.load(filehandle)\n","\n","  ## Retrieve Lbels\n","  with open('dataset_files/TrainLabels', 'rb') as filehandle:\n","      # store the encoded documents as binary data\n","      y_train = pickle.load(filehandle)\n","\n","  with open('dataset_files/TestLabels', 'rb') as filehandle:\n","      # store the encoded documents as binary data\n","      y_test = pickle.load(filehandle)\n","  \n","  # Check encodings dimensions\n","  encodings_dim = X_train[0].shape[1]\n","\n","  # Turn Xtrain an X_test into ndarrays\n","  X_train = np.concatenate(X_train, axis=0)\n","  X_test  = np.concatenate(X_test, axis=0)\n","\n","  # Scaling\n","  if scaling == 'StandardScaling':\n","    print('Standard scaling.')\n","    from sklearn.preprocessing import StandardScaler\n","    scaler = StandardScaler()\n","    scaler.fit(X_train)\n","    X_train = scaler.transform(X_train)\n","    X_test  = scaler.transform(X_test)\n","  elif scaling == 'MinMaxScaling':\n","    print('MinMax scaling.')\n","    from sklearn.preprocessing import MinMaxScaler\n","    scaler = MinMaxScaler()\n","    scaler.fit(X_train)\n","    X_train = scaler.transform(X_train)\n","    X_test  = scaler.transform(X_test)\n","  else:\n","    print('No scaling applied.')\n","\n","  print('\\nComplete')\n","  print('{} training instances\\n {} test instances'.format(len(X_train), len(X_test)))\n","  print('\\nencodings_dim = {}'.format(encodings_dim))\n","\n","  return(X_train, y_train, X_test, y_test)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TX-acH1N48nw"},"source":["# Loading encoded Tweets and labels"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blhjKy4dxlP9","executionInfo":{"status":"ok","timestamp":1607648417084,"user_tz":360,"elapsed":5859,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"ecbe7a7d-9574-4999-b58a-72ab62d00bee"},"source":["encoding = 'W2V1'\n","scaling = 'StandardScaling'\n","\n","data = load_encoded_corpus(encoding, scaling)\n","encodings_dim = data[0].shape[1]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W2V 1 - Spanish Unannotated Corpora\n","Encoded tweets and labels are being loaded.\n","Standard scaling.\n","\n","Complete\n","4500 training instances\n"," 500 test instances\n","\n","encodings_dim = 100\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TTy2BSVvQE5D"},"source":["# Talos Testing"]},{"cell_type":"markdown","metadata":{"id":"j9YT02F5RcCo"},"source":["## Parameters dictionary"]},{"cell_type":"code","metadata":{"id":"OUdIAghf24ma","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607679326564,"user_tz":360,"elapsed":2519,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"e5591961-0313-4216-d29a-76e6bade6a2b"},"source":["from tensorflow.keras.activations import relu, elu\r\n","\r\n","p = {\r\n","    'first_layer_size': [500, 400, 300, 200, 100],                    #5\r\n","    'second_layer_size': [200, 150, 125, 100, 75, 50],                #6\r\n","    'activation':['relu', 'elu'],                                     #2\r\n","    'p_dropout': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],                 #7\r\n","    'batch_size': [32, 128, 256, 512, 768, 1024],                     #6\r\n","    'epochs':[10, 20, 30, 40],                                        #4\r\n","    'optimizer':['adam','rmsprop']                                    #2\r\n","}\r\n","\r\n","search_space_size = 1\r\n","for v in p.values():\r\n","  search_space_size *= len(v)\r\n","\r\n","print('{} permutations in the search space.'.format(search_space_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["20160 permutations in the search space.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6wd9KUAYIQ5Q"},"source":["## Model definition"]},{"cell_type":"code","metadata":{"id":"A2i_LQYD1THp"},"source":["# add input parameters to the function\r\n","import tensorflow as tf\r\n","from tensorflow.keras.models import Sequential\r\n","from tensorflow.keras.layers import Dense, Activation, Dropout\r\n","\r\n","def test_model(x_train, y_train, x_val, y_val, params):\r\n","    \r\n","    # replace the hyperparameter inputs with references to params dictionary \r\n","    #imput_layer_size, h1_size = params['layers_size']\r\n","    activation_f = params['activation']\r\n","\r\n","    model = Sequential()\r\n","    model.add(Dense(units = params['first_layer_size'], activation=activation_f))\r\n","    model.add(Dense(units=params['second_layer_size'], activation=activation_f))\r\n","    model.add(Dropout(rate = params['p_dropout']))\r\n","    model.add(Dense(1, activation='sigmoid'))\r\n","    model.compile(loss='binary_crossentropy', optimizer=params['optimizer'], metrics=['acc'])\r\n","    \r\n","    # make sure history object is returned by model.fit()\r\n","    out = model.fit(x=x_train, \r\n","                    y=y_train,\r\n","                    validation_data=(x_val, y_val),\r\n","                    epochs=params['epochs'],\r\n","                    batch_size=params['batch_size'],\r\n","                    verbose=0)\r\n","    \r\n","    # modify the output model\r\n","    return out, model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ge8uUIbrIVaq"},"source":["## Scanning"]},{"cell_type":"code","metadata":{"id":"WUOc4JMq4hbN"},"source":["import talos\r\n","x_train, y_train, x_val, y_val = data\r\n","t = talos.Scan(x=x_train, y=y_train, params=p, \r\n","               model=test_model, \r\n","               experiment_name='SNN_experiment_1', \r\n","               x_val = x_val, \r\n","               y_val=y_val,\r\n","               random_method='quantum',\r\n","               fraction_limit = 0.1, print_params = True)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JP8l9M6aIdjl"},"source":["## Results analisys"]},{"cell_type":"code","metadata":{"id":"RAhUu054D8sI"},"source":["t.data.sort_values(by='val_acc', ascending=False).head(30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtIrDtscABp-"},"source":["r = Reporting('experiment_log.csv')\r\n","\r\n","# returns the results dataframe\r\n","r.data\r\n","\r\n","# returns the highest value for 'val_fmeasure'\r\n","r.high('val_fmeasure')\r\n","\r\n","# returns the number of rounds it took to find best model\r\n","r.rounds2high()\r\n","\r\n","# draws a histogram for 'val_acc'\r\n","r.plot_hist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IDJ2TNIiEjSN"},"source":["# Resources\n","\n","* Mini batch size selection: [1](https://datascience.stackexchange.com/questions/18414/are-there-any-rules-for-choosing-the-size-of-a-mini-batch), [2](https://www.quora.com/In-deep-learning-why-dont-we-use-the-whole-training-set-to-compute-the-gradient), [3](https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu)\n","* Optimizers: [1](https://ai.stackexchange.com/questions/18206/what-kind-of-optimizer-is-suggested-to-use-for-binary-classification-of-similar)\n","* RMSprop: [1](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a)\n","* Number of Hidden Layers: [1](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)\n","\n","    \n"]}]}