{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EVALUATIONS_OVER_VALIDATION_SET.ipynb","provenance":[],"collapsed_sections":["LniwSmIxBut7","3iPKAv_BCcHb"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RfekjF9mkdWy"},"source":["# Ajustes iniciales"]},{"cell_type":"markdown","metadata":{"id":"zgdSkhxZdv0_"},"source":["## ConexiÃ³n a google drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rEA8wkz2HzN","outputId":"e9ebbbc4-a71b-4d00-a159-db525673d26d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Crq_ww9d2GNh","outputId":"6de85e09-a4b2-40e3-9842-f7637e0565d4"},"source":["%cd ./drive/MyDrive/Colab Notebooks/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: './drive/MyDrive/Colab Notebooks/'\n","/content/drive/MyDrive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Wm9BK0WAxy_N","outputId":"d973baab-75c5-4347-fcce-4e58e2ab7684"},"source":["%pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Colab Notebooks'"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"8fz70fVKJJ11"},"source":["## Importar modulos *_utils\n"]},{"cell_type":"code","metadata":{"id":"Vm7v_xFpU3sr"},"source":["import sys\n","sys.path.append('/content/drive/My Drive/Colab Notebooks/')\n","\n","from my_utils import eval_utils\n","from my_utils import nn_utils\n","from my_utils import dataset_utils\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tr4J9es5dPjH"},"source":["# Local Utils"]},{"cell_type":"markdown","metadata":{"id":"1LPJhrOnwtE5"},"source":["# EVALUATE TRAINED MODELS (TEST SET)"]},{"cell_type":"markdown","metadata":{"id":"SUW76L3Nd0DN"},"source":["## Trained Models Evaluations"]},{"cell_type":"markdown","metadata":{"id":"pGKa7AQyQZ0w"},"source":["##evaluateOnVal()"]},{"cell_type":"code","metadata":{"id":"s5N-ogC3c4EI"},"source":["import pickle\n","from pandas import DataFrame\n","\n","def evaluateOnVal(task, search_results_path, architecture):\n","  ## Load test data\n","  print('Loading test data...')\n","  X_train, Y_train = loadTrainData(task, architecture)\n","  print()\n","\n","  ## Load search results\n","  with open(search_results_path, 'rb') as file_handler:\n","    search_results = pickle.load(file_handler)\n","\n","  ## evaluate for method-A pondering (global mean)\n","  print('Evaluating method A models')\n","  search_results.sort_values(by='val_acc_A', ascending=False, inplace=True)\n","  config_ids = search_results.conf_ID[:5].to_list()\n","\n","  merged_results_1 = DataFrame()\n","\n","  \n","\n","  for config_id in config_ids[:]:\n","    config_metrics_df = evalModelsOnValidation(X_train, Y_train, \n","                                               task, \n","                                               config_id, \n","                                               'A', \n","                                               f'{architecture}-1', 7)\n","    \n","    merged_results_1=merged_results_1.append(config_metrics_df, ignore_index=True)\n","\n","  results_file = f'./Results/final/{architecture}-1_{task}_VALIDATION.df'\n","\n","  with open(results_file, 'wb') as file_handler:\n","    pickle.dump(merged_results_1, file_handler)\n","\n","  print(f'\\nResults saved to: {results_file}')\n","  print()\n","\n","  ## evaluate for method-B pondering (per-fold-analysys)\n","  print('Evaluating method B models')\n","\n","  search_results.sort_values(by='val_acc_B', ascending=False, inplace=True)\n","  config_ids = search_results.conf_ID[:5].to_list()\n","\n","  merged_results_2 = DataFrame()\n","\n","  for config_id in config_ids[:]:\n","    config_metrics_df = evalModelsOnValidation(X_train, Y_train, \n","                                               task, \n","                                               config_id, \n","                                               'B', \n","                                               f'{architecture}-2', 7)\n","    \n","    merged_results_2=merged_results_2.append(config_metrics_df, ignore_index=True)\n","\n","  results_file = f'./Results/final/{architecture}-2_{task}_VALIDATION.df'\n","\n","  with open(results_file, 'wb') as file_handler:\n","    pickle.dump(merged_results_2, file_handler)\n","\n","  print(f'\\nResults saved to: {results_file}')\n","\n","  return merged_results_1, merged_results_2\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVsZkayB_b9b"},"source":["def loadTrainData(task, architecture):\n","  if architecture == 'SNN':\n","    encoding_format = 'SINGLE-VEC'\n","  else:\n","    encoding_format = 'EMB-SEQ'\n","\n","  X_train, Y_train = dataset_utils.loadEncodedTrainData(embedding_type='FT3',\n","                                                 encoding_format=encoding_format,\n","                                                 labels_to_return=[task],\n","                                                 n_folds=7)\n","  \n","  return X_train, Y_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ye5q-DY-eNsF"},"source":["### evaluateTrainedModelsOnValidationData()"]},{"cell_type":"code","metadata":{"id":"2VaYFP-peNsG"},"source":["\n","from pandas import DataFrame\n","from  tensorflow.keras.utils import to_categorical\n","\n","def evalModelsOnValidation(X_train, Y_train, task, config_ID, \n","                                          eval_method, arch_label, n_folds):\n","\n","  evaluations_record = list()\n","\n","  # EVALUATE THE FOLDS CLASSIFIERS ---------------------------------------------\n","  for fold_idx in range(n_folds):\n","    #print('\\nEvaluating data-fold {}'.format(fold_idx))\n","  \n","    weights_file = f'F{fold_idx}_{eval_method}.hdf5'\n","    trained_model = loadPretrainedModel(config_ID, weights_file)\n","\n","    # DATA\n","    x_val   = X_train[fold_idx]\n","\n","    # LABELS\n","    val_mask = Y_train['kfold'] == fold_idx\n","    y_val   = Y_train.loc[val_mask, :]\n","\n","    # make predictions on x_val samples\n","    classes_probs = trained_model.predict(x_val)\n","    \n","    # turn the prob distributions into classes predictions \n","    labels_predictions_array = getClassesPredictions(classes_probs, task)\n","\n","    # evaluate the preditions\n","    evaluation = evaluatePredictions(task, y_val[task], labels_predictions_array)\n","    \n","    model_results_dict = {'conf_id': config_ID,\n","                          'model_type': 'F',\n","                          'architecture': arch_label,\n","                          **evaluation}\n","\n","    evaluations_record.append(model_results_dict) \n","\n","  print('*', end='')\n","\n","  evaluations_results_df = DataFrame(evaluations_record)\n","\n","  return evaluations_results_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJYO5vQQd0DR"},"source":["### loadPretrainedModel()"]},{"cell_type":"code","metadata":{"id":"qZbtm8kCd0DS"},"source":["#*************************     loadPretrainedModel()    ************************  \n","from keras.models import model_from_json\n","\n","def loadPretrainedModel(config_ID, weights_file):\n","  # load model configuration from json file\n","  json_file = open(f'./models_json_files/{config_ID}.json', 'r')\n","  model_config = json_file.read()\n","  json_file.close()\n","  trained_model = model_from_json(model_config)\n","\n","  # load pretrained weights into the model\n","  trained_model.load_weights(f'./trained_models/{config_ID}/{weights_file}')\n","\n","  return trained_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jxk7mtkOd0DT"},"source":["### getClassesPredictions()"]},{"cell_type":"code","metadata":{"id":"R4iNGiQdd0Dc"},"source":["import numpy as np\n","\n","def class_pred(true_prob):\n","  if true_prob>=0.5:\n","    return 1\n","  else:\n","    return 0\n","\n","def getClassesPredictions(classes_probs, task):\n","  if task=='HTA': \n","    return np.array([probs.argmax() for probs in classes_probs]).reshape(-1,1)\n","  else:\n","    return np.apply_along_axis(class_pred, 1, classes_probs).reshape(-1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DkMGjcm8d0Dd"},"source":["### evaluatePredictions()"]},{"cell_type":"code","metadata":{"id":"nvqpCQ4Ud0Df"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","def evaluatePredictions(task, val_labels, pred_labels, verbose=False):\n","\n","  if task=='HTA':\n","\n","    # get the correspondig predicted and val (target) labels for each task\n","    pred_HS_labels, pred_TR_labels, pred_AG_labels = dataset_utils.getLabelsPerTask(pred_labels)\n","    val_HS_labels, val_TR_labels, val_AG_labels = dataset_utils.getLabelsPerTask(val_labels)\n","\n","    # compute the different metrics\n","    HS_acc = accuracy_score(val_HS_labels, pred_HS_labels)\n","    HS_prec = precision_score(val_HS_labels, val_HS_labels, average=\"macro\")\n","    HS_recall = recall_score(val_HS_labels, val_HS_labels, average=\"macro\")\n","    HS_f1 = f1_score(val_HS_labels, pred_HS_labels, average=\"macro\")\n","\n","    AG_acc = accuracy_score(val_AG_labels, pred_AG_labels)\n","    AG_prec = precision_score(val_AG_labels, pred_AG_labels, average=\"macro\")\n","    AG_recall = recall_score(val_AG_labels, pred_AG_labels, average=\"macro\")\n","    AG_f1 = f1_score(val_AG_labels, pred_AG_labels, average=\"macro\")\n","\n","    TR_acc = accuracy_score(val_TR_labels, pred_TR_labels)\n","    TR_prec = precision_score(val_TR_labels, pred_TR_labels, average=\"macro\")\n","    TR_recall = recall_score(val_TR_labels, pred_TR_labels, average=\"macro\")\n","    TR_f1 = f1_score(val_TR_labels, pred_TR_labels, average=\"macro\")\n","\n","    F1_multi = (HS_f1+ AG_f1 + TR_f1)/3\n","\n","    EMR = computeEMR(list(zip(val_HS_labels, val_TR_labels, val_AG_labels)),\n","                    list(zip(pred_HS_labels, pred_TR_labels, pred_AG_labels)))\n","\n","    results_dict = {'HS_acc':HS_acc,\n","                    'HS_prec':HS_prec,\n","                    'HS_recall':HS_recall,\n","                    'HS_f1':HS_f1,\n","                    'AG_acc':AG_acc,\n","                    'AG_prec':AG_prec,\n","                    'AG_recall':AG_recall,\n","                    'AG_f1':AG_f1,\n","                    'TR_acc':TR_acc,\n","                    'TR_prec':TR_prec,\n","                    'TR_recall':TR_recall,\n","                    'TR_f1':TR_f1,\n","                    'F1_multi':F1_multi,\n","                    'EMR':EMR}\n","\n","    if verbose:\n","      print('EMR = ', EMR)\n","      print('F1_multi = ', F1_multi)\n","      print()\n","\n","    return results_dict\n","\n","  if task in ['HS', 'TR', 'AG']:\n","    # compute the different metrics\n","    acc = accuracy_score(val_labels, pred_labels)\n","    prec = precision_score(val_labels, pred_labels, average=\"macro\")\n","    recall = recall_score(val_labels, pred_labels, average=\"macro\")\n","    f1_macro = f1_score(val_labels, pred_labels, average=\"macro\")\n","\n","    results_dict = {'acc':acc,\n","                    'prec':prec,\n","                    'recall':recall,\n","                    'f1-macro':f1_macro}\n","\n","    if verbose:\n","      print('Acc = ', acc)\n","      print('F1_macro = ', f1_macro)\n","      print()\n","\n","    return results_dict\n","\n","def computeEMR(test_labels, pred_labels):\n","  total_instances = len(test_labels)\n","  exact_match_count= 0\n","  for gold, pred in zip(test_labels, pred_labels):\n","    #print(gold, pred)\n","    if gold == pred:\n","      exact_match_count += 1\n","\n","  return exact_match_count/total_instances\n","\n","def compute_metrics(target, predicted):\n","    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","    accuracy = accuracy_score(target, predicted)\n","    precision = precision_score(target, predicted, average=\"macro\")\n","    recall = recall_score(target, predicted, average=\"macro\")\n","    f1 = f1_score(val_labels, pred_labels, average=\"macro\")\n","\n","    results = {'acc':accuracy_s, \n","              'prec' : precision_pos,\n","              'recall' : precision_neg,  \n","              'f1': recall_pos,\n","              'recall_neg' : recall_neg,\n","              'f1_pos': f1_pos,\n","              'f1_neg': f1_neg}\n","    \n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"87-lC8pyNpDs"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","def evaluatePredictions(task, val_labels, pred_labels, verbose=False):\n","\n","  if task=='HTA':\n","\n","    # get the correspondig predicted and val (target) labels for each task\n","    pred_HS_labels, pred_TR_labels, pred_AG_labels = dataset_utils.getLabelsPerTask(pred_labels)\n","    val_HS_labels, val_TR_labels, val_AG_labels = dataset_utils.getLabelsPerTask(val_labels)\n","\n","    # compute ACC, PREC, RECALL and F1 metrics\n","    HS_acc, HS_prec, HS_recall, HS_f1 = compute_metrics(val_HS_labels, pred_HS_labels)\n","    AG_acc, AG_prec, AG_recall, AG_f1 = compute_metrics(val_AG_labels, pred_AG_labels)   \n","    TR_acc, TR_prec, TR_recall, TR_f1 = compute_metrics(val_TR_labels, pred_TR_labels)\n","\n","    F1_multi = (HS_f1+ AG_f1 + TR_f1)/3\n","\n","    EMR = computeEMR(list(zip(val_HS_labels, val_TR_labels, val_AG_labels)),\n","                    list(zip(pred_HS_labels, pred_TR_labels, pred_AG_labels)))\n","\n","    results_dict = {'HS_acc':HS_acc,\n","                    'HS_prec':HS_prec,\n","                    'HS_recall':HS_recall,\n","                    'HS_f1':HS_f1,\n","                    'AG_acc':AG_acc,\n","                    'AG_prec':AG_prec,\n","                    'AG_recall':AG_recall,\n","                    'AG_f1':AG_f1,\n","                    'TR_acc':TR_acc,\n","                    'TR_prec':TR_prec,\n","                    'TR_recall':TR_recall,\n","                    'TR_f1':TR_f1,\n","                    'F1_multi':F1_multi,\n","                    'EMR':EMR}\n","\n","    if verbose:\n","      print('EMR = ', EMR)\n","      print('F1_multi = ', F1_multi)\n","      print()\n","\n","    return results_dict\n","\n","  if task in ['HS', 'TR', 'AG']:\n","    # compute ACC, PREC, RECALL and F1 metrics\n","    acc, prec, recall, f1_macro = compute_metrics(val_labels, pred_labels)\n","\n","    results_dict = {'acc':acc,\n","                    'prec':prec,\n","                    'recall':recall,\n","                    'f1-macro':f1_macro}\n","\n","    if verbose:\n","      print('Acc = ', acc)\n","      print('F1_macro = ', f1_macro)\n","      print()\n","\n","    return results_dict\n","\n","def compute_metrics(target, predicted):\n","    accuracy = accuracy_score(target, predicted)\n","    precision = precision_score(target, predicted, average=\"macro\")\n","    recall = recall_score(target, predicted, average=\"macro\")\n","    f1 = f1_score(target, predicted, average=\"macro\")\n","    \n","    return accuracy, precision, recall, f1    \n","\n","def computeEMR(test_labels, pred_labels):\n","  total_instances = len(test_labels)\n","  exact_match_count= 0\n","  for gold, pred in zip(test_labels, pred_labels):\n","    #print(gold, pred)\n","    if gold == pred:\n","      exact_match_count += 1\n","\n","  return exact_match_count/total_instances"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uer7Lz3id0Dg"},"source":["### labels_utils"]},{"cell_type":"code","metadata":{"id":"E-t-4QHWd0Dg"},"source":["#**************************     getLabelsPerTask()    **************************\n","def getLabelsPerTask(HTA_labels):\n","\n","    HS_labels = list()\n","    TR_labels = list()\n","    AG_labels = list()\n","\n","    for HTA_label in HTA_labels:\n","        HS_label, TR_label, AG_label = mapTo3DimsFormat(HTA_label)\n","\n","        HS_labels.append(HS_label)\n","        TR_labels.append(TR_label)\n","        AG_labels.append(AG_label)\n","\n","    HS_labels = np.array(HS_labels).reshape(-1,1)\n","    TR_labels = np.array(TR_labels).reshape(-1,1)\n","    AG_labels = np.array(AG_labels).reshape(-1,1)\n","\n","    return (HS_labels, TR_labels, AG_labels)\n","\n","#**************************     mapTo3DimsFormat()    ************************** \n","def mapTo3DimsFormat(AB_label):\n","  '''\n","  Maps label in five_classes_format to 3 dims labeling.\n","\n","    0 -> (0,0,0)  [HT = 0, TR = 0, AG = 0]\n","    1 -> (1,0,0)  [HT = 1, TR = 0, AG = 0]\n","    2 -> (1,0,1)  [HT = 1, TR = 0, AG = 1]\n","    3 -> (1,1,0)  [HT = 1, TR = 1, AG = 0]\n","    4 -> (1,1,1)  [HT = 1, TR = 1, AG = 1]\n","\n","  inpunt:\n","  label    - int, label in five_classes_format\n","\n","  output:\n","  (H,T,A)  - ints tuple, labeling in 3 dims format\n","\n","  '''\n","  if AB_label == 0:\n","    return(0,0,0)\n","\n","  elif AB_label == 1:\n","    return(1,0,0)\n","\n","  elif AB_label == 2:\n","    return(1,0,1)\n","\n","  elif AB_label == 3:\n","    return(1,1,0)\n","\n","  elif AB_label == 4:\n","    return(1,1,1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y-VJNYhwHHqr"},"source":["# EVALUATE TRAINED MODELS ON VALIDATION DATA"]},{"cell_type":"markdown","metadata":{"id":"-5FtNNFfEKBV"},"source":["#SNN (CMPLETE)"]},{"cell_type":"markdown","metadata":{"id":"xDKdEWgtEMOE"},"source":["## HS"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2EdK2Q9EC_XJ","outputId":"220b8caf-721e-4a72-9833-087008a524df"},"source":["results_A, results_B = evaluateOnVal('HS', \n","                          search_results_path = './Results/SNN/HS/experiments_4.df',\n","                          architecture = 'SNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: SINGLE-VEC\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (300,)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/SNN-1_HS_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/SNN-2_HS_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HKL-qsXfGvAm"},"source":["## AG"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cx6lTUwxGvBM","outputId":"8e38b4e9-f50a-48c2-d258-153529439278"},"source":["results_A, results_B = evaluateOnVal('AG', \n","                          search_results_path = './Results/SNN/AG/experiments_4.df',\n","                          architecture = 'SNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: SINGLE-VEC\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (300,)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/SNN-1_AG_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/SNN-2_AG_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CJLNAa86EYe6"},"source":["## TR"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O7DwXaOWEYfA","outputId":"b743ec8e-a9ee-4d69-b339-215253f0094d"},"source":["results_A, results_B = evaluateOnVal('TR', \n","                          search_results_path = './Results/SNN/TR/experiments_4.df',\n","                          architecture = 'SNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: SINGLE-VEC\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (300,)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/SNN-1_TR_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/SNN-2_TR_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r-j7pb-5dyil"},"source":["## HTA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqpcLSrBdyi0","outputId":"9e25e353-b671-4f38-e7f6-cf2eab1ceba1"},"source":["results_A, results_B = evaluateOnVal('HTA', \n","                          search_results_path = './Results/SNN/HTA/experiments_4f.df',\n","                          architecture = 'SNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: SINGLE-VEC\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (300,)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/SNN-1_HTA_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/SNN-2_HTA_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vm1IhlVXijTY"},"source":["# CNN (COMPLETE)"]},{"cell_type":"markdown","metadata":{"id":"7ko05XgFijTn"},"source":["## HS"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2vJfdV9SijTp","outputId":"66ad91a5-922f-4e12-ac7e-2459d02e37db"},"source":["results_A, results_B = evaluateOnVal('HS', \n","                          search_results_path = './Results/CNN/HS/experiments_7f.df',\n","                          architecture = 'CNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/CNN-1_HS_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/CNN-2_HS_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yaLm7VKiXPvP"},"source":["## AG"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0J-gKp_HXPvd","outputId":"b899a871-8da4-410d-e119-87c5fe6c3bac"},"source":["results_A, results_B = evaluateOnVal('AG', \n","                          search_results_path = './Results/CNN/AG/experiments_6f.df',\n","                          architecture = 'CNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/CNN-1_AG_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/CNN-2_AG_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KGuxvzMuijTz"},"source":["## TR"]},{"cell_type":"code","metadata":{"id":"CzUSjBvkijT0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be08e95d-9a13-48d9-a684-8e9c6569ae66"},"source":["results_A, results_B = evaluateOnVal('TR', \n","                          search_results_path = './Results/CNN/TR/experiments_6f.df',\n","                          architecture = 'CNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/CNN-1_TR_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/CNN-2_TR_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yGuNxriyXPvh"},"source":["## HTA"]},{"cell_type":"code","metadata":{"id":"ls0Ez8KVXPvi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca62608a-c77d-4bb8-9280-e7dab9360d50"},"source":["results_A, results_B = evaluateOnVal('HTA', \n","                          search_results_path = './Results/CNN/HTA/experiments_6f.df',\n","                          architecture = 'CNN')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/CNN-1_HTA_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/CNN-2_HTA_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K4i_v9vhQgto"},"source":["# BiLSTM (COMPLETE)"]},{"cell_type":"markdown","metadata":{"id":"D94DPmc6Qgtp"},"source":["## HS"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OEVP9I_Qgtq","outputId":"6dcc6e83-0794-4983-ac83-0b452cd8de57"},"source":["results_A, results_B = evaluateOnVal('HS', \n","                      search_results_path = './Results/BiLSTM/HS/experiments_4f.df',\n","                      architecture = 'BiLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/BiLSTM-1_HS_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/BiLSTM-2_HS_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_sZ_MMKAQgtu"},"source":["## AG"]},{"cell_type":"code","metadata":{"id":"H_D1TPNCQgtu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6cdb536f-44a3-4fad-e1c6-f0792a51d323"},"source":["results_A, results_B = evaluateOnVal('AG', \n","                          search_results_path = './Results/BiLSTM/AG/experiments_4f.df',\n","                          architecture = 'BiLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/BiLSTM-1_AG_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/BiLSTM-2_AG_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ESdC3FCLQgts"},"source":["## TR"]},{"cell_type":"code","metadata":{"id":"vi4HJeJZQgts","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b51afc53-59c4-4b29-dee4-419b8fca74e3"},"source":["results_A, results_B = evaluateOnVal('TR', \n","                          search_results_path = './Results/BiLSTM/TR/experiments_4f.df',\n","                          architecture = 'BiLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/BiLSTM-1_TR_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/BiLSTM-2_TR_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HbicdY29Qgtw"},"source":["## HTA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y4JG_1qUQgtw","outputId":"fa188637-d7bc-46ff-ffa5-3004fc46b179"},"source":["results_A, results_B = evaluateOnVal('HTA', \n","                          search_results_path = './Results/BiLSTM/HTA/experiments_4f.df',\n","                          architecture = 'BiLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/BiLSTM-1_HTA_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/BiLSTM-2_HTA_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TPosaQBOtHf7"},"source":["# ConvLSTM (COMPLETE)"]},{"cell_type":"markdown","metadata":{"id":"ivA9b0eAtHgV"},"source":["## HS"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cn0pir4YtHgY","outputId":"cb770d5a-bbd4-4ca0-ad77-bf8cf319f672"},"source":["results_A, results_B = evaluateOnVal('HS', \n","                      search_results_path = './Results/ConvLSTM/HS/experiments_5f.df',\n","                      architecture = 'ConvLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/ConvLSTM-1_HS_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/ConvLSTM-2_HS_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o0S9sZNWtHgg"},"source":["## AG"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s15pXB-dtHgi","outputId":"ec55e4ec-6071-435e-c92f-52bffb0545a4"},"source":["results_A, results_B = evaluateOnVal('AG', \n","                          search_results_path = './Results/ConvLSTM/AG/experiments_5f.df',\n","                          architecture = 'ConvLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/ConvLSTM-1_AG_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/ConvLSTM-2_AG_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BF4CJtxhtHgl"},"source":["## TR"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uOgdorvutHgn","outputId":"e2b3467b-3502-44c2-c0f9-b63a093c0b01"},"source":["results_A, results_B = evaluateOnVal('TR', \n","                          search_results_path = './Results/ConvLSTM/TR/experiments_5f.df',\n","                          architecture = 'ConvLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/ConvLSTM-1_TR_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/ConvLSTM-2_TR_VALIDATION.df\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XO7a31mXtHgp"},"source":["## HTA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LF4zCRwAtHgr","outputId":"9ad15692-aae7-4d4c-8952-759d3eb8827b"},"source":["results_A, results_B = evaluateOnVal('HTA', \n","                          search_results_path = './Results/ConvLSTM/HTA/experiments_5f.df',\n","                          architecture = 'ConvLSTM')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading test data...\n","FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: EMB-SEQ\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (55, 300)\n","\n","Evaluating method A models\n","*****\n","Results saved to: ./Results/final/ConvLSTM-1_HTA_VALIDATION.df\n","\n","Evaluating method B models\n","*****\n","Results saved to: ./Results/final/ConvLSTM-2_HTA_VALIDATION.df\n"],"name":"stdout"}]}]}