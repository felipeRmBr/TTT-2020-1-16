{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN - EMBEDDINGS SELECTION EXPERIMENTS - (ALFA PRIME).ipynb","provenance":[{"file_id":"1nCjbEs1SzSNt2tSoPe4-IWMkvxhYrMvz","timestamp":1610696308550}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"q5EmzImyiRQR"},"source":["# Setting Up"]},{"cell_type":"markdown","metadata":{"id":"zgdSkhxZdv0_"},"source":["## Conexión a google drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rEA8wkz2HzN","executionInfo":{"status":"ok","timestamp":1622754744374,"user_tz":300,"elapsed":38798,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"d6d58714-0d3a-4d68-e1e5-03242ac9dce1"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6P-ir_64f1xV","executionInfo":{"status":"ok","timestamp":1622754760153,"user_tz":300,"elapsed":243,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"ab9c4ffc-1da7-4a52-d3a2-08c1421bc8e7"},"source":["%cd ./drive/MyDrive/Colab Notebooks"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"TvUQpQ5khUhX","executionInfo":{"status":"ok","timestamp":1622754760459,"user_tz":300,"elapsed":48,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"e8bddaaa-11ac-40d5-a80e-cef732743f6c"},"source":["%pwd"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Colab Notebooks'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"8bNlkg-_9jQA"},"source":["## Importar los módulos *_utils"]},{"cell_type":"code","metadata":{"id":"hcHzLcXz9jQB","executionInfo":{"status":"ok","timestamp":1622754878255,"user_tz":300,"elapsed":351,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}}},"source":["import sys\n","sys.path.append('/content/drive/My Drive/Colab Notebooks/')\n","from my_utils import dataset_utils\n","from my_utils import eval_utils\n","from my_utils import nn_utils"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"o3hau2tv9jQC","executionInfo":{"status":"ok","timestamp":1622754878441,"user_tz":300,"elapsed":15,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}}},"source":["import pickle"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uJi6IoaKU2mD"},"source":["# Local Utils"]},{"cell_type":"markdown","metadata":{"id":"YkqdkYv1kRr7"},"source":["### fcNeuralNetworkModel()"]},{"cell_type":"code","metadata":{"id":"qWeX7bLpkRsA","executionInfo":{"status":"ok","timestamp":1622754878443,"user_tz":300,"elapsed":14,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}}},"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","def fcNeuralNetworkModel(task, params, optimizer, verbose = False):\n","    \"\"\"\n","    Defines and compiles a new Keras model for a Fully Connected Neural Network\n","    with 1 hidden layer (a total of 3 layers) and dropout regularization.\n","    The number of units per layer, the activation function, the regularization \n","    rate and the are given by the params dictionary.\n","\n","    \"\"\"\n","    VEC_EMBEDDINGS_DIM = 300\n","    activation_f = params['activation']\n","\n","    input_vector = keras.Input(name='INPUT', shape=(VEC_EMBEDDINGS_DIM), dtype=\"float64\")\n","\n","    x  = layers.Dense(units=params['L1_size'], \n","                              activation=activation_f, \n","                              name = 'DL_1')(input_vector)\n","\n","    x = layers.Dropout(rate=params['p_dropout_1'], name = 'DROPOUT_1')(x)                              \n","\n","    x = layers.Dense(units=params['L2_size'], \n","                              activation=activation_f, \n","                              name = 'DL_2')(x)                          \n","\n","    x = layers.Dropout(rate=params['p_dropout_2'], name = 'DROPOUT_2')(x)\n","\n","    if task in ['HS', 'TR', 'AG']:\n","      preds = layers.Dense(1, activation=\"sigmoid\", name = 'PREDICTOR')(x)\n","      model = keras.Model(input_vector, preds)\n","\n","      model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n","\n","    elif task == 'HTA':\n","      preds = layers.Dense(5, activation=\"softmax\", name = 'PREDICTOR')(x)\n","      preds = reshape = layers.Reshape(target_shape=(5,))(preds)\n","\n","      model = keras.Model(input_vector, preds)\n","      model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n","\n","    if verbose:\n","      model.summary()\n","\n","    return model"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gQO_ey81H9H3"},"source":["# External Utils"]},{"cell_type":"markdown","metadata":{"id":"shHzGfmfIMdr"},"source":["### merged cells"]},{"cell_type":"code","metadata":{"id":"eazEiPKSIMeD"},"source":["# **************************   spaceScanner()   ************************** \n","\n","# adapted from Talos solution\n","# (check: https://autonomio.github.io/talos/#/)\n","import pickle\n","from itertools import product\n","from random import sample\n","from pandas import DataFrame\n","from numpy import random as np_random\n","\n","def spaceScanner(X_train, Y_train, task, model_prototype, search_space, arch_params_keys, \n","                training_params_keys, fraction2eval, fitting_attemps, n_folds, stop_threshold=0.99, \n","                partial_CV=False, backup_file='testing_file.df', backup_freq=25, \n","                save_history_files=False, save_models_as_json=False):\n","  \"\"\"\n","  Evaluates a sample of the configurations defined in the search space.\n","\n","  inputs:\n","  X_train               - list[numpy_array], encoded folds with the train data\n","  Y_train               - DataFrame, labels (Y_train.columns = ['HS','TR','AG','HTA','kfold'])\n","  task                  - str, valid_task_options = ['HS','TR','AG','HTA']\n","  model_prototype       - function, model prototype definition (returns compiled keras model)\n","  search_space          - list[tuple], combinations of parameters to be evaluated \n","                          (Every entry in the search space is a two elements tuple, \n","                          the first of this elements is a list of architecture_params and the \n","                          second one is a list of training_params)\n","  arch_params_keys      - list[str], arch_params_keys\n","  training_params_keys  - list[str], training_params_keys\n","  evaluation_mode       - int, eithe 1 or 2. See footnote for a description about how each one of\n","                          this works\n","  fitting_attemps       - int, try fitting the model N times and keep the best result (N=fitting_attemps)\n","  n_folds               - int, number of folds for cross validation\n","  partial_CV            - boolean, partial_CV=True means that the model should only be evaluated \n","                          in half of the n_folds\n","  backup_file           - str, name of the file where the experiment results will ve saved\n","                          (The filename should be passed without extension. A .df extension will be appended)\n","  backup_freq           - int, how often will the eperiment results be backed-up\n","  ---------\n","\n","\n","  outputs:\n","  results_df            - DataFrame, one row for every configuration evaluated\n","\n","  \"\"\"\n","  print(f'BackupFile: {backup_file}')\n","  print('SCANNING SEARCH SPACE\\n')\n","\n","  # take a sample of the search_space\n","  if fraction2eval < 1:\n","    sample_size = int(len(search_space)*fraction2eval)\n","    # space_sample=sample(search_space, sample_size)\n","  else: \n","    sample_size = len(search_space)\n","    # space_sample=search_space\n","\n","  \"\"\"\n","  #When using space samples\n","  # transform the parameters-tuples into parameters-dictionaries\n","  space_sample = [[dict( zip(arch_params_keys, params_combo[0]) ),\n","                   dict( zip(training_params_keys, params_combo[1]) )] for params_combo in space_sample]\n","  \"\"\"\n","\n","  # when using random variable\n","  search_space = [[dict( zip(arch_params_keys, params_combo[0]) ),\n","                   dict( zip(training_params_keys, params_combo[1]) )] for params_combo in search_space]\n","\n","  # validate the configurations in the search_space_sample\n","  results_list = []\n","\n","  for config_idx, conf_params in enumerate(search_space):\n","    # use a binomial random variable to decide weatther to test the \n","    # current configuration or pass. The parameter p of the binomial \n","    # experiment is iqual to fraction2eval\n","    random_v = np_random.binomial(1,fraction2eval)\n","    if random_v == 0:\n","      continue\n","\n","\n","    # get a ramdom conf_ID\n","    conf_ID = get_random_string(8)\n","\n","    # print a configurations count\n","    #counter_str = '{}/{}'.format(config_count + 1, sample_size).ljust(9,' ') \n","    #print(counter_str, end = '')\n","    print(f'conf_ID: {conf_ID}   ', end = '')\n","\n","    # evaluate the current config using croos validation\n","    arch_params, training_params = conf_params\n","\n","    validation_results = modelCrossValidation(X_train, Y_train,\n","                                             task,  \n","                                             model_prototype,\n","                                             arch_params,\n","                                             training_params,\n","                                             n_folds,\n","                                             fitting_attemps,\n","                                             partial_CV,\n","                                             conf_ID,\n","                                             save_history_files,\n","                                             save_models_as_json)\n","\n","    # update the results list\n","    results_list.append(validation_results)\n","\n","    # display progress and results\n","    train_acc_A = validation_results['train_acc_A']\n","    val_acc_A   = validation_results['val_acc_A']\n","    train_acc_B = validation_results['train_acc_B']\n","    val_acc_B   = validation_results['val_acc_B']\n","\n","    #progress_string    = '[aprox. {}% complete]'.format(round(100*(config_count+1)/sample_size,2)).ljust(20, ' ')\n","\n","    train_acc_A_str = 'train_acc_A = {},'.format(round(train_acc_A,3)).ljust(20,' ')\n","    val_acc_A_str   = 'val_acc_A = {}'.format(round(val_acc_A,3)).ljust(20,' ')\n","    train_acc_B_str = 'train_acc_B = {},'.format(round(train_acc_B,3)).ljust(20,' ')\n","    val_acc_B_str   = 'val_acc_B = {}'.format(round(val_acc_B,3)).ljust(20,' ')\n","\n","    print('--  {}  {} - {}  {}'.format(train_acc_A_str, \n","                                val_acc_A_str,\n","                                train_acc_B_str,\n","                                val_acc_B_str))\n","\n","    # save results every X configurations\n","    if len(results_list)%backup_freq == 0:\n","      partial_backup_file = f'{backup_file}.partial'\n","      with open(partial_backup_file, 'wb') as file_handler:\n","        pickle.dump(DataFrame(results_list), file_handler)\n","\n","  # final backup\n","  results_df = DataFrame(results_list)\n","\n","  complete_backup_file = backup_file\n","  with open(complete_backup_file, 'wb') as file_handler:\n","    pickle.dump(results_df, file_handler)\n","\n","  print(f'\\nDONE IS COMPLETE. {len(results_list)} CONFIGURATIONS WERE SUCCESFULLY EVALUATED.')\n","\n","  return results_df\n","\n","\n","# **************************   modelCrossValidation()   ************************** \n","\n","def modelCrossValidation(X_train,Y_train,task,model_prototype,arch_params,training_params,\n","                         n_folds,fitting_attemps,partial_CV,configuration_ID,save_history_files,\n","                         save_model_as_json):\n","\n","    \"\"\"\n","    Evaluates a model configuration using cross validation. \n","\n","    inputs:\n","    X_train               - list[numpy_array], encoded folds with the train data\n","    Y_train               - DataFrame, labels (Y_train.columns = ['HS','TR','AG','HTA','kfold'])\n","    task                  - str, valid_task_options = ['HS','TR','AG','HTA']\n","    model_prototype       - function obj, model prototype definition (returns compiled keras model)\n","    arch_params           - dict, arch_params_keys\n","    training_params       - dict, training_params_keys\n","    n_folds               - int, number of folds for cross validation\n","    fitting_attemps       - int, the model should be fitted N=fitting_attemps times for every fold\n","    partial_CV            - boolean, partial_CV=True means that the model should only be evaluated \n","                            in half of the folds\n","    configuration_ID      - str, 8 char ID\n","    save_history_files    - boolean, should history_files be backed up or not\n","    save_model_as_json    - boolean\n","\n","    \n","    outputs:\n","    evaluation_results_dict   - dict, summarizes the results of the cross validations             \n","\n","    \"\"\"\n","\n","    save_trained_models = False\n","\n","    # unpack the training parameters\n","    optimizer_id = training_params['optimizer']\n","    max_epochs = training_params['max_epochs']\n","    batch_size = training_params['batch_size']\n","\n","    # instantiate the corresponding optimizer\n","    optimizer    = optimizers_list[optimizer_id]\n","\n","    if save_model_as_json:\n","      model = model_prototype(task, arch_params, optimizer)\n","      model_json = model.to_json()\n","      with open(f\"./models_json_files/{configuration_ID}.json\", \"w\") as json_file:\n","        json_file.write(model_json)\n","\n","    # The main work ------------------------------------------------------------\n","    histories_list = list()   # list[dict]\n","\n","    for fold_idx, data_fold in enumerate(getDataSplitsGenerator(X_train, Y_train, n_folds)):\n","      \n","      # When partial_CV==True just evaluate the model in half of the n-folds\n","      if (partial_CV) and (fold_idx%2==1):\n","        continue\n","\n","      # Run N attemps to fit the model (N=fitting_attemps)\n","      # save the model_history of the model with the best val_acc metric\n","      best_try_history = None\n","      max_acc = 0\n","      for i in range(fitting_attemps):\n","\n","        model = model_prototype(task, arch_params, optimizer)\n","\n","        acc, model_history = fitNeuralNetworkModel(model, task, (data_fold), batch_size, \n","                                                   max_epochs)\n","\n","        if acc > max_acc:\n","          max_acc = acc\n","          best_attemp = i\n","          best_try_history = model_history\n","\n","      # append the best_try_history to the list of histories\n","      histories_list.append(best_try_history)\n","      \n","      # print a hint at the end of every fold validation\n","      print('*', end='')\n","\n","    # compute a summary of the metrics\n","    metrics_summary = getMetricsSummary(histories_list)\n","\n","    # form a dictionary to report the validation results\n","    validation_results_dict = {'conf_ID':configuration_ID}\n","    validation_results_dict.update(arch_params)\n","    validation_results_dict.update(training_params)\n","    validation_results_dict.update(metrics_summary)\n","\n","    # save the list of histories for future reference\n","    if save_history_files:\n","        with open(f'./history_files/{configuration_ID}.dict', 'wb') as file_handler:\n","            pickle.dump(histories_list, file_handler)\n","  \n","    print('  ', end='')\n","\n","    return validation_results_dict\n","\n","\n","# **************************   fitNeuralNetworkModel()   **************************\n","\n","def fitNeuralNetworkModel(model, task, data, batch_size, max_epochs, verbose=0):\n","  \"\"\"\n","  Fits a precompiled keras model to the given data. \n","\n","  inputs:\n","  model           - compiled keras model\n","  task            - str, ['HS','TR', 'AG', 'HTA']\n","  data            - tuple, (x_train, y_train, x_val, y_val)\n","  batch_size      - int \n","  max_epochs      - int\n","  stop_threshold  - float, 0 < stop_threshold < 1\n","  \n","  --\n","  x_train         - numpy_array [shape = N_t, ENCODDING_DIM]\n","  y_train         - numpy_array [shape = N_t, 1]\n","  x_val           - numpy_array [shape = N_v, ENCODDING_DIM]\n","  y_val           - numpy_array [shape = N_v, 1]\n","\n","  outputs:\n","  max_acc         - float, max_val_acc registered in model_history\n","  model_history   - dict, model.history.history\n","\n","  \"\"\"\n","\n","  # Callbacks\n","  \"\"\"\n","  stopping_condition = MyStoppingCallback(1)\n","  callbacks_list = [stopping_condition]\n","\n","  if cp_filepath != 'null_path':\n","    checkpoint = tf.keras.callbacks.ModelCheckpoint(cp_filepath, \n","                                                    monitor='val_acc', \n","                                                    verbose=0, \n","                                                    save_best_only=True, \n","                                                    mode='max')\n","    \n","    callbacks_list.append(checkpoint)\n","  \"\"\"\n","\n","  # Main work\n","  x_train, y_train, x_val, y_val = data \n","\n","  # Extract the labels that correspond to the given task\n","  # type(y_train) = type(y_val) = pandas.DataFrame\n","  y_train = y_train[task]\n","  y_val   = y_val[task]\n","\n","  # For multiclass classification we need to transform \n","  # the labels to a one-hot-encoding representation\n","  if task == 'HTA':\n","    y_train = tf.keras.utils.to_categorical(y_train, num_classes=5)\n","    y_val = tf.keras.utils.to_categorical(y_val, num_classes=5)     \n","\n","  model.fit(x=x_train, \n","            y=y_train,\n","            validation_data=(x_val, y_val),\n","            epochs=max_epochs,\n","            batch_size=batch_size,\n","            verbose=verbose)\n","  \n","  max_acc = max(model.history.history['val_acc'])\n","\n","  return max_acc, model.history.history\n","\n","\n","# **************************   getMetricsSummary()   ************************** \n","from statistics import mean\n","\n","def getMetricsSummary(hist_list):\n","    \"\"\"\n","    Returns a summary of the metrics registered in the histories_list.\n","\n","    inputs:\n","    hist_list       - list[model.history.history], list of models histories \n","\n","    output:\n","    metrics_summary - dict, keys = [train_acc_A, val_acc_A, train_acc_B, val_acc_B] \n","\n","    #### Two methosd to evaluate every configuration:\n","\n","    #### Method A:\n","        1) Compute the mean-val-acc for every epoch\n","        2) Let E be the epoch with the hihgest mean-val-acc \n","        3) Report both the mean-val-acc and mean-train-acc at epoch E\n","\n","    #### Method B:\n","        1) Let E(k) be the epoch with the highest val-acc recorded for fold k\n","        1) Let val_acc(E(k)) and train_acc(E(k)) be the metrics regitered for fold k at the epoch E(K)\n","        2) Report the mean values of both val_acc(k) and acc(k)\n","\n","\n","    \"\"\"   \n","\n","    best_epoch, train_acc_A, val_acc_A = method_A_Metrics(hist_list)\n","    min2best, max2best, train_acc_B, val_acc_B = method_B_Metrics(hist_list)\n","\n","    return {'best_epochh':best_epoch,\n","            'train_acc_A':train_acc_A, \n","            'val_acc_A':val_acc_A,\n","            'min2best':min2best,\n","            'max2best':max2best,\n","            'train_acc_B':train_acc_B,\n","            'val_acc_B':val_acc_B\n","            }\n","\n","\n","# **************************   computeGlobalMetrics()   **************************\n","\n","def method_A_Metrics(histories_list):\n","    # Method A results:\n","    mean_val_acc  = list()     \n","    mean_train_acc = list() \n","\n","    # Find M (the largest mean-val-acc)\n","    max_mean_val_acc = 0\n","    acc_zip = list(zip(*[h['acc'] for h in histories_list]))\n","    val_acc_zip = list(zip(*[h['val_acc'] for h in histories_list]))\n","\n","    n_epochs = len(acc_zip)\n","    best_epoch_idx = 0\n","    for i in range(n_epochs):\n","        mean_train_acc.append(mean(acc_zip[i]))\n","        mean_val_acc.append(mean(val_acc_zip[i]))\n","\n","        if mean_val_acc[-1] > max_mean_val_acc:\n","            max_mean_val_acc = mean_val_acc[-1]\n","            best_epoch_idx = i\n","\n","    \"\"\"\n","    DELTA = 0  \n","    DELTA = .5/100\n","\n","    if DELTA > 0:\n","        # Try to find an epoch with a smaller stdev\n","        # Use M to pick the best epoch\n","        best_epoch_val_stdev = 1\n","        best_epoch_idx = 0\n","\n","        for i in range(n_epochs):\n","            val_acc = mean_val_acc[i]\n","            val_stdev = val_acc_stdev[i]\n","\n","            if (val_acc >= max_val_acc - DELTA) & (val_stdev < best_epoch_val_stdev):\n","                best_epoch_val_stdev = val_stdev\n","                best_epoch_idx = i\n","    \"\"\"\n","\n","    return best_epoch_idx, mean_train_acc[best_epoch_idx], mean_val_acc[best_epoch_idx]\n","\n","def method_B_Metrics(histories_list):\n","    # Method B results:\n","    val_acc_results  = []\n","    train_acc_results = []\n","\n","    best_epochs_list = []\n","    for h in histories_list:\n","        val_acc_history = h['val_acc']\n","        train_acc_history = h['acc']\n","\n","        fold_max_val_acc = max(val_acc_history)\n","        fold_best_epoch = val_acc_history.index(fold_max_val_acc)\n","        best_epochs_list.append(fold_best_epoch)\n","\n","        val_acc_results.append(fold_max_val_acc)\n","        train_acc_results.append(train_acc_history[fold_best_epoch])\n","\n","    min2best = min(best_epochs_list)\n","    max2best = max(best_epochs_list)\n","\n","    return min2best, max2best, mean(train_acc_results), mean(val_acc_results)\n","\n","\n","# **************************   getDataSplitsGenerator()   **************************\n","from numpy import concatenate as np_concatenate\n","\n","def getDataSplitsGenerator(X_train, Y_train, n_folds):\n","  '''\n","  Splits train data into K folds for cross validation. (K=n_folds)\n","\n","  inputs:\n","  X_train   - list[numpy_array], encoded folds\n","  Y_train   - DataFrame, containing the labels for the training instances \n","  n_folds   - int, number of folds for cross validation\n","\n","  -------\n","  inputs preconditions:\n","  len(X_train) == n_folds\n","\n","  outputs:\n","  (x_train, y_train, x_val, y_val) - generator with K different splits of the train data\n","\n","  '''\n","\n","  for k in range(n_folds):\n","  \n","    # DATA\n","    x_train = np_concatenate([X_train[i] for i in range(n_folds) if i not in [k]], axis=0)\n","    x_val   = X_train[k]\n","\n","    # LABELS\n","    train_mask = Y_train['kfold'] != k\n","    val_mask = Y_train['kfold'] == k\n","\n","    y_train = Y_train.loc[train_mask, :]\n","    y_val   = Y_train.loc[val_mask, :]\n","\n","    yield (x_train, y_train, x_val, y_val)\n","\n","\n","# **************************   get_random_string()   **************************\n","from random import choice\n","import string\n","\n","def get_random_string(length):\n","    # choose from all lowercase+uppercase letters\n","    letters = string.ascii_lowercase + string.ascii_uppercase\n","    result_str = ''.join(choice(letters) for i in range(length))\n","\n","    return result_str    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zmfZJyYL2Q3a"},"source":["### Miscelaneous"]},{"cell_type":"code","metadata":{"id":"FYzAFDXyx7YK"},"source":["# **************************   getDataSplitsGenerator()   **************************\n","from numpy import concatenate as np_concatenate\n","\n","def getDataSplitsGenerator(X_train, Y_train, n_folds):\n","  '''\n","  Splits train data into K folds for cross validation. (K=n_folds)\n","\n","  inputs:\n","  X_train   - list[numpy_array], encoded folds\n","  Y_train   - DataFrame, containing the labels for the training instances \n","  n_folds   - int, number of folds for cross validation\n","\n","  -------\n","  inputs preconditions:\n","  len(X_train) == n_folds\n","\n","  outputs:\n","  (x_train, y_train, x_val, y_val) - generator with K different splits of the train data\n","\n","  '''\n","\n","  for k in range(n_folds):\n","  \n","    # DATA\n","    x_train = np_concatenate([X_train[i] for i in range(n_folds) if i not in [k]], axis=0)\n","    x_val   = X_train[k]\n","\n","    # LABELS\n","    train_mask = Y_train['kfold'] != k\n","    val_mask = Y_train['kfold'] == k\n","\n","    y_train = Y_train.loc[train_mask, :]\n","    y_val   = Y_train.loc[val_mask, :]\n","\n","    yield (x_train, y_train, x_val, y_val)\n","\n","\n","# **************************   get_random_string()   **************************\n","from random import choice\n","import string\n","\n","def get_random_string(length):\n","    # choose from all lowercase+uppercase letters\n","    letters = string.ascii_lowercase + string.ascii_uppercase\n","    result_str = ''.join(choice(letters) for i in range(length))\n","\n","    return result_str    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VzmHED3kdGpR"},"source":["### optimizers_list"]},{"cell_type":"code","metadata":{"id":"MZ90qKhCdLum"},"source":["import tensorflow.keras.optimizers as keras_optimizers\n","\n","optimizers_list = {'adam-1e-3':keras_optimizers.Adam(learning_rate=0.001),                          #adam_1\n","              'adam-7e-4':keras_optimizers.Adam(learning_rate=0.0007),\n","              'adam-5e-4':keras_optimizers.Adam(learning_rate=0.0005),\n","              'adam-3e-4':keras_optimizers.Adam(learning_rate=0.0003),\n","              'adam-1e-4':keras_optimizers.Adam(learning_rate=0.0001),\n","              'rmsprop-1e-3':keras_optimizers.RMSprop(learning_rate=0.001, momentum=0.0),           # rmsprop_1\n","              'rmsprop-7e-4':keras_optimizers.RMSprop(learning_rate=0.0007, momentum=0.0),\n","              'rmsprop-5e-4':keras_optimizers.RMSprop(learning_rate=0.0005, momentum=0.0),\n","              'rmsprop-3e-4':keras_optimizers.RMSprop(learning_rate=0.0003, momentum=0.0),\n","              'rmsprop-1e-4':keras_optimizers.RMSprop(learning_rate=0.0001, momentum=0.0),\n","              'rmsprop-7.5e-5':keras_optimizers.RMSprop(learning_rate=0.000075, momentum=0.0),      # rmsprop_7\n","              'rmsprop-5e-5':keras_optimizers.RMSprop(learning_rate=0.00005, momentum=0.0),\n","              'rmsprop-1e-3-mu0.9':keras_optimizers.RMSprop(learning_rate=0.001, momentum=0.9),\n","              'rmsprop-7e-4-mu0.9':keras_optimizers.RMSprop(learning_rate=0.0007, momentum=0.9),\n","              'rmsprop-5e-4-mu0.9':keras_optimizers.RMSprop(learning_rate=0.0005, momentum=0.9),\n","              'rmsprop-3e-4-mu0.9':keras_optimizers.RMSprop(learning_rate=0.0003, momentum=0.9),\n","              'rmsprop-1e-4-mu0.9':keras_optimizers.RMSprop(learning_rate=0.0001, momentum=0.9),\n","              'rmsprop-7.5e-5-mu0.9':keras_optimizers.RMSprop(learning_rate=0.000075, momentum=0.9), \n","              'rmsprop-5e-5-mu0.9':keras_optimizers.RMSprop(learning_rate=0.00005, momentum=0.9)}    # rmsprop_7"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0vezcH52Wly"},"source":["### spaceScanner()"]},{"cell_type":"code","metadata":{"id":"PNw8OFLKyIJ2"},"source":["# **************************   spaceScanner()   ************************** \n","\n","# adapted from Talos solution\n","# (check: https://autonomio.github.io/talos/#/)\n","from itertools import product\n","from random import sample\n","from pandas import DataFrame\n","import pickle\n","from numpy import random as np_random\n","\n","def spaceScanner(X_train, Y_train, task, model_prototype, search_space, arch_params_keys, \n","                training_params_keys, fraction2eval, fitting_attemps, n_folds, stop_threshold=0.99, \n","                partial_CV=False, backup_file='testing_file.df', backup_freq=25, \n","                save_history_files=False, save_models_as_json=False):\n","  \"\"\"\n","  Evaluates a sample of the configurations defined in the search space.\n","\n","  inputs:\n","  X_train               - list[numpy_array], encoded folds with the train data\n","  Y_train               - DataFrame, labels (Y_train.columns = ['HS','TR','AG','HTA','kfold'])\n","  task                  - str, valid_task_options = ['HS','TR','AG','HTA']\n","  model_prototype       - function, model prototype definition (returns compiled keras model)\n","  search_space          - list[tuple], combinations of parameters to be evaluated \n","                          (Every entry in the search space is a two elements tuple, \n","                          the first of this elements is a list of architecture_params and the \n","                          second one is a list of training_params)\n","  arch_params_keys      - list[str], arch_params_keys\n","  training_params_keys  - list[str], training_params_keys\n","  evaluation_mode       - int, eithe 1 or 2. See footnote for a description about how each one of\n","                          this works\n","  fitting_attemps       - int, try fitting the model N times and keep the best result (N=fitting_attemps)\n","  n_folds               - int, number of folds for cross validation\n","  partial_CV            - boolean, partial_CV=True means that the model should only be evaluated \n","                          in half of the n_folds\n","  backup_file           - str, name of the file where the experiment results will ve saved\n","                          (The filename should be passed without extension. A .df extension will be appended)\n","  backup_freq           - int, how often will the eperiment results be backed-up\n","  ---------\n","\n","\n","  outputs:\n","  results_df            - DataFrame, one row for every configuration evaluated\n","\n","  \"\"\"\n","  print(f'BackupFile: {backup_file}')\n","  print('SCANNING SEARCH SPACE\\n')\n","\n","  # take a sample of the search_space\n","  if fraction2eval < 1:\n","    sample_size = int(len(search_space)*fraction2eval)\n","    # space_sample=sample(search_space, sample_size)\n","  else: \n","    sample_size = len(search_space)\n","    # space_sample=search_space\n","\n","  \"\"\"\n","  #When using space samples\n","  # transform the parameters-tuples into parameters-dictionaries\n","  space_sample = [[dict( zip(arch_params_keys, params_combo[0]) ),\n","                   dict( zip(training_params_keys, params_combo[1]) )] for params_combo in space_sample]\n","  \"\"\"\n","\n","  # when using random variable\n","  search_space = [[dict( zip(arch_params_keys, params_combo[0]) ),\n","                   dict( zip(training_params_keys, params_combo[1]) )] for params_combo in search_space]\n","\n","  # validate the configurations in the search_space_sample\n","  results_list = []\n","\n","  for config_idx, conf_params in enumerate(search_space):\n","    # use a binomial random variable to decide weatther to test the \n","    # current configuration or pass. The parameter p of the binomial \n","    # experiment is iqual to fraction2eval\n","    random_v = np_random.binomial(1,fraction2eval)\n","    if random_v == 0:\n","      continue\n","\n","\n","    # get a ramdom conf_ID\n","    conf_ID = get_random_string(8)\n","\n","    # print a configurations count\n","    #counter_str = '{}/{}'.format(config_count + 1, sample_size).ljust(9,' ') \n","    #print(counter_str, end = '')\n","    print(f'conf_ID: {conf_ID}   ', end = '')\n","\n","    # evaluate the current config using croos validation\n","    arch_params, training_params = conf_params\n","\n","    validation_results = modelCrossValidation(X_train, Y_train,\n","                                             task,  \n","                                             model_prototype,\n","                                             arch_params,\n","                                             training_params,\n","                                             n_folds,\n","                                             fitting_attemps,\n","                                             partial_CV,\n","                                             conf_ID,\n","                                             save_history_files,\n","                                             save_models_as_json)\n","\n","    # update the results list\n","    results_list.append(validation_results)\n","\n","    # display progress and results\n","    train_acc_A = validation_results['train_acc_A']\n","    val_acc_A   = validation_results['val_acc_A']\n","    train_acc_B = validation_results['train_acc_B']\n","    val_acc_B   = validation_results['val_acc_B']\n","\n","    #progress_string    = '[aprox. {}% complete]'.format(round(100*(config_count+1)/sample_size,2)).ljust(20, ' ')\n","\n","    train_acc_A_str = 'train_acc_A = {},'.format(round(train_acc_A,3)).ljust(20,' ')\n","    val_acc_A_str   = 'val_acc_A = {}'.format(round(val_acc_A,3)).ljust(20,' ')\n","    train_acc_B_str = 'train_acc_B = {},'.format(round(train_acc_B,3)).ljust(20,' ')\n","    val_acc_B_str   = 'val_acc_B = {}'.format(round(val_acc_B,3)).ljust(20,' ')\n","\n","    print('--  {}  {} - {}  {}'.format(train_acc_A_str, \n","                                val_acc_A_str,\n","                                train_acc_B_str,\n","                                val_acc_B_str))\n","\n","    # save results every X configurations\n","    if len(results_list)%backup_freq == 0:\n","      partial_backup_file = f'{backup_file}.partial'\n","      with open(partial_backup_file, 'wb') as file_handler:\n","        pickle.dump(DataFrame(results_list), file_handler)\n","\n","  # final backup\n","  results_df = DataFrame(results_list)\n","\n","  complete_backup_file = backup_file\n","  with open(complete_backup_file, 'wb') as file_handler:\n","    pickle.dump(results_df, file_handler)\n","\n","  print(f'\\nDONE IS COMPLETE. {len(results_list)} CONFIGURATIONS WERE SUCCESFULLY EVALUATED.')\n","\n","  return results_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9NzAX5YL3sr8","executionInfo":{"status":"ok","timestamp":1616567071299,"user_tz":360,"elapsed":12644,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"4063b1fa-a44c-4cff-9f5d-50898b8a76fb"},"source":["len('[aprox. 100.00% complete]')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"AhQqDTLi2a5M"},"source":["### modelCrossValidation()"]},{"cell_type":"code","metadata":{"id":"2bH5tCSoxHHD"},"source":["# **************************   modelCrossValidation()   ************************** \n","\n","import os\n","\n","import tensorflow.keras.optimizers as keras_optimizers\n","\n","def modelCrossValidation(X_train,Y_train,task,model_prototype,arch_params,training_params,\n","                         n_folds,fitting_attemps,partial_CV,configuration_ID,save_history_files,\n","                         save_model_as_json):\n","\n","    \"\"\"\n","    Evaluates a model configuration using cross validation. \n","\n","    inputs:\n","    X_train               - list[numpy_array], encoded folds with the train data\n","    Y_train               - DataFrame, labels (Y_train.columns = ['HS','TR','AG','HTA','kfold'])\n","    task                  - str, valid_task_options = ['HS','TR','AG','HTA']\n","    model_prototype       - function obj, model prototype definition (returns compiled keras model)\n","    arch_params           - dict, arch_params_keys\n","    training_params       - dict, training_params_keys\n","    n_folds               - int, number of folds for cross validation\n","    fitting_attemps       - int, the model should be fitted N=fitting_attemps times for every fold\n","    partial_CV            - boolean, partial_CV=True means that the model should only be evaluated \n","                            in half of the folds\n","    configuration_ID      - str, 8 char ID\n","    save_history_files    - boolean, should history_files be backed up or not\n","    save_model_as_json    - boolean\n","\n","    \n","    outputs:\n","    evaluation_results_dict   - dict, summarizes the results of the cross validations             \n","\n","    \"\"\"\n","\n","    save_trained_models = False\n","\n","    # unpack the training parameters\n","    optimizer_id = training_params['optimizer']\n","    max_epochs = training_params['max_epochs']\n","    batch_size = training_params['batch_size']\n","\n","    # instantiate the corresponding optimizer\n","    optimizer    = optimizers_list[optimizer_id]\n","\n","    if save_model_as_json:\n","      model = model_prototype(task, arch_params, optimizer)\n","      model_json = model.to_json()\n","      with open(f\"./models_json_files/{configuration_ID}.json\", \"w\") as json_file:\n","        json_file.write(model_json)\n","\n","    # The main work ------------------------------------------------------------\n","    histories_list = list()   # list[dict]\n","\n","    for fold_idx, data_fold in enumerate(getDataSplitsGenerator(X_train, Y_train, n_folds)):\n","      \n","      # When partial_CV==True just evaluate the model in half of the n-folds\n","      if (partial_CV) and (fold_idx%2==1):\n","        continue\n","\n","      # Run N attemps to fit the model (N=fitting_attemps)\n","      # save the model_history of the model with the best val_acc metric\n","      best_try_history = None\n","      max_acc = 0\n","      for i in range(fitting_attemps):\n","\n","        model = model_prototype(task, arch_params, optimizer)\n","\n","        acc, model_history = fitNeuralNetworkModel(model, task, (data_fold), batch_size, \n","                                                   max_epochs)\n","\n","        if acc > max_acc:\n","          max_acc = acc\n","          best_attemp = i\n","          best_try_history = model_history\n","\n","      # append the best_try_history to the list of histories\n","      histories_list.append(best_try_history)\n","      \n","      # print a hint at the end of every fold validation\n","      print('*', end='')\n","\n","    # compute a summary of the metrics\n","    metrics_summary = getMetricsSummary(histories_list)\n","\n","    # form a dictionary to report the validation results\n","    validation_results_dict = {'conf_ID':configuration_ID}\n","    validation_results_dict.update(arch_params)\n","    validation_results_dict.update(training_params)\n","    validation_results_dict.update(metrics_summary)\n","\n","    # save the list of histories for future reference\n","    if save_history_files:\n","        with open(f'./history_files/{configuration_ID}.dict', 'wb') as file_handler:\n","            pickle.dump(histories_list, file_handler)\n","  \n","    print('  ', end='')\n","\n","    return validation_results_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4cz6y0sb4UYX"},"source":["### fitNeuralNet()"]},{"cell_type":"code","metadata":{"id":"EOpO1qZ9zraw"},"source":["# **************************   fitNeuralNetworkModel()   **************************\n","\n","def fitNeuralNetworkModel(model, task, data, batch_size, max_epochs, verbose=0):\n","  \"\"\"\n","  Fits a precompiled keras model to the given data. \n","\n","  inputs:\n","  model           - compiled keras model\n","  task            - str, ['HS','TR', 'AG', 'HTA']\n","  data            - tuple, (x_train, y_train, x_val, y_val)\n","  batch_size      - int \n","  max_epochs      - int\n","  stop_threshold  - float, 0 < stop_threshold < 1\n","  \n","  --\n","  x_train         - numpy_array [shape = N_t, ENCODDING_DIM]\n","  y_train         - numpy_array [shape = N_t, 1]\n","  x_val           - numpy_array [shape = N_v, ENCODDING_DIM]\n","  y_val           - numpy_array [shape = N_v, 1]\n","\n","  outputs:\n","  max_acc         - float, max_val_acc registered in model_history\n","  model_history   - dict, model.history.history\n","\n","  \"\"\"\n","\n","  # Callbacks\n","  \"\"\"\n","  stopping_condition = MyStoppingCallback(1)\n","  callbacks_list = [stopping_condition]\n","\n","  if cp_filepath != 'null_path':\n","    checkpoint = tf.keras.callbacks.ModelCheckpoint(cp_filepath, \n","                                                    monitor='val_acc', \n","                                                    verbose=0, \n","                                                    save_best_only=True, \n","                                                    mode='max')\n","    \n","    callbacks_list.append(checkpoint)\n","  \"\"\"\n","\n","  # Main work\n","  x_train, y_train, x_val, y_val = data \n","\n","  # Extract the labels that correspond to the given task\n","  # type(y_train) = type(y_val) = pandas.DataFrame\n","  y_train = y_train[task]\n","  y_val   = y_val[task]\n","\n","  # For multiclass classification we need to transform \n","  # the labels to a one-hot-encoding representation\n","  if task == 'HTA':\n","    y_train = tf.keras.utils.to_categorical(y_train, num_classes=5)\n","    y_val = tf.keras.utils.to_categorical(y_val, num_classes=5)     \n","\n","  model.fit(x=x_train, \n","            y=y_train,\n","            validation_data=(x_val, y_val),\n","            epochs=max_epochs,\n","            batch_size=batch_size,\n","            verbose=verbose)\n","  \n","  max_acc = max(model.history.history['val_acc'])\n","\n","  return max_acc, model.history.history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EwAIYC8g4YNk"},"source":["### getMetricsSummary()"]},{"cell_type":"code","metadata":{"id":"tF4AVhNvydh3"},"source":["from statistics import mean\n","\n","def getMetricsSummary(hist_list):\n","    \"\"\"\n","    Returns a summary of the metrics registered in the histories_list.\n","\n","    inputs:\n","    hist_list       - list[model.history.history], list of models histories \n","\n","    output:\n","    metrics_summary - dict, keys = [train_acc_A, val_acc_A, train_acc_B, val_acc_B] \n","\n","    # Two methosd to evaluate every configuration:\n","\n","    # Method A:\n","        1) Compute the mean-val-acc for every epoch\n","        2) Let E be the epoch with the hihgest mean-val-acc \n","        3) Report both the mean-val-acc and mean-train-acc at epoch E\n","\n","    # Method B:\n","        1) Let E_k be the epoch with the highest val-acc recorded for fold k\n","        2) For each fold extract the val_acc and train_acc at epoch E_k\n","        3) Compute the mean of the extracted-metrics\n","    \"\"\"   \n","\n","    best_epoch, train_acc_A, val_acc_A = method_A_Metrics(hist_list)\n","    min2best, max2best, train_acc_B, val_acc_B = method_B_Metrics(hist_list)\n","\n","    return {'best_epochh':best_epoch,\n","            'train_acc_A':train_acc_A, \n","            'val_acc_A':val_acc_A,\n","            'min2best':min2best,\n","            'max2best':max2best,\n","            'train_acc_B':train_acc_B,\n","            'val_acc_B':val_acc_B\n","            }\n","\n","# **************************   computeGlobalMetrics()   **************************\n","\n","def method_A_Metrics(histories_list):\n","    # Method A results:\n","    mean_val_acc  = list()     \n","    mean_train_acc = list() \n","\n","    # Find M (the largest mean-val-acc)\n","    max_mean_val_acc = 0\n","    acc_zip = list(zip(*[h['acc'] for h in histories_list]))\n","    val_acc_zip = list(zip(*[h['val_acc'] for h in histories_list]))\n","\n","    n_epochs = len(acc_zip)\n","    best_epoch_idx = 0\n","    for i in range(n_epochs):\n","        mean_train_acc.append(mean(acc_zip[i]))\n","        mean_val_acc.append(mean(val_acc_zip[i]))\n","\n","        if mean_val_acc[-1] > max_mean_val_acc:\n","            max_mean_val_acc = mean_val_acc[-1]\n","            best_epoch_idx = i\n","\n","    \"\"\"\n","    DELTA = 0  \n","    DELTA = .5/100\n","\n","    if DELTA > 0:\n","        # Try to find an epoch with a smaller stdev\n","        # Use M to pick the best epoch\n","        best_epoch_val_stdev = 1\n","        best_epoch_idx = 0\n","\n","        for i in range(n_epochs):\n","            val_acc = mean_val_acc[i]\n","            val_stdev = val_acc_stdev[i]\n","\n","            if (val_acc >= max_val_acc - DELTA) & (val_stdev < best_epoch_val_stdev):\n","                best_epoch_val_stdev = val_stdev\n","                best_epoch_idx = i\n","    \"\"\"\n","\n","    return best_epoch_idx, mean_train_acc[best_epoch_idx], mean_val_acc[best_epoch_idx]\n","\n","def method_B_Metrics(histories_list):\n","    # Method B results:\n","    val_acc_results  = []\n","    train_acc_results = []\n","\n","    best_epochs_list = []\n","    for h in histories_list:\n","        val_acc_history = h['val_acc']\n","        train_acc_history = h['acc']\n","\n","        fold_max_val_acc = max(val_acc_history)\n","        fold_best_epoch = val_acc_history.index(fold_max_val_acc)\n","        best_epochs_list.append(fold_best_epoch)\n","\n","        val_acc_results.append(fold_max_val_acc)\n","        train_acc_results.append(train_acc_history[fold_best_epoch])\n","\n","    min2best = min(best_epochs_list)\n","    max2best = max(best_epochs_list)\n","\n","    return min2best, max2best, mean(train_acc_results), mean(val_acc_results)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O_GjKxTdBEDQ"},"source":["# EXPERIMENTS"]},{"cell_type":"markdown","metadata":{"id":"eJzYHOGN2exY"},"source":["Ejecutar las siguientes operaciones para cada uno de los diferentes encodings ['FT1', 'FT2', 'FT3', 'W2V300', 'GloVe300']"]},{"cell_type":"markdown","metadata":{"id":"u2yf18rVfuQW"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O7ca7Qa6fuQX","executionInfo":{"status":"ok","timestamp":1622758175951,"user_tz":300,"elapsed":3262,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"9a9e2f01-b460-4924-8dd5-6b8dcc72651b"},"source":["X_train, Y_train = dataset_utils.loadEncodedTrainData(embedding_type='FT2',\n","                                                 encoding_format='SINGLE-VEC',\n","                                                 labels_to_return=['HS'],\n","                                                 n_folds=7)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["FastText 2 - Esp. Wikipedia\n","Encoding Format: SINGLE-VEC\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (300,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FYs9sy_AW58v"},"source":["## Parameters dictionary"]},{"cell_type":"markdown","metadata":{"id":"4xQlfIryfAFq"},"source":["### Architecture params combos"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i04H0tjFfkHU","executionInfo":{"status":"ok","timestamp":1622754890107,"user_tz":300,"elapsed":17,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"72d1aa03-c089-4242-f11f-24104e8aedf3"},"source":["from itertools import product\n","\n","arch_params_dict = {\n","    'L1_size': [300, 200],                    \n","    'L2_size': [150, 100, 75],                \n","    'activation':['relu'],                                    \n","    'p_dropout_1':[0, 0.5],\n","    'p_dropout_2':[0, 0.5]                                \n","}\n","\n","arch_params_combos = list(product( *arch_params_dict.values() ))          \n","\n","arch_params_combos = [params_combo for params_combo in arch_params_combos if params_combo[-1]==params_combo[-2]]\n","print('{} arch_params_combos.'.format(len(arch_params_combos)))   "],"execution_count":16,"outputs":[{"output_type":"stream","text":["12 arch_params_combos.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yeWlUp3meJCn"},"source":["### Training params combinations"]},{"cell_type":"code","metadata":{"id":"JoLM2TFGedA2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622754890347,"user_tz":300,"elapsed":40,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"d3fc805f-dcc4-4516-ba19-9b02119e37f9"},"source":["trainig_params_dict = {'optimizer':['adam-1e-3', 'adam-5e-4', 'adam-1e-4', 'rmsprop-1e-3', 'rmsprop-5e-4', 'rmsprop-1e-4'],\n","                       'batch_size':[256,512,1024],\n","                       'max_epochs':[75]}\n","                        \n","\n","trainig_params_combos = list(product( *trainig_params_dict.values() ))\n","\n","print('\\n{} trainig_params_combos.'.format(len(trainig_params_combos))) "],"execution_count":17,"outputs":[{"output_type":"stream","text":["\n","18 trainig_params_combos.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVx3ZpXJf3Eb","executionInfo":{"status":"ok","timestamp":1622754890349,"user_tz":300,"elapsed":34,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"f0e0ebe3-0597-408d-b7ff-7ca495b15ee4"},"source":["search_space = list(product(arch_params_combos, trainig_params_combos))\n","\n","print('\\n{} params combinations in the search_space.'.format(len(search_space))) "],"execution_count":18,"outputs":[{"output_type":"stream","text":["\n","216 params combinations in the search_space.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_8PxFdeZf2F_"},"source":["## Escaneo"]},{"cell_type":"code","metadata":{"id":"0hjDqnp9f2GB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622761306056,"user_tz":300,"elapsed":2343916,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"d3aee678-d4b4-4f45-af6a-412afd65fcbc"},"source":["results_df = nn_utils.spaceScanner(X_train, Y_train,\n","                          'HS',\n","                          fcNeuralNetworkModel, \n","                          search_space[:],\n","                          arch_params_dict.keys(),\n","                          trainig_params_dict.keys(),\n","                          fraction2eval=1,\n","                          n_folds=7,\n","                          fitting_attemps=1,\n","                          stop_threshold=1,\n","                          partial_CV=True,\n","                          backup_file='./Results/SNN/FT2_embeddings_experiments.df',\n","                          backup_freq=25,\n","                          save_history_files=False,\n","                          save_models_as_json=False)\n","\n","                          # CNN_HTA_EXT_DATA_NEW_EXPERIMENTS"],"execution_count":21,"outputs":[{"output_type":"stream","text":["BackupFile: ./Results/SNN/FT2_embeddings_experiments.df\n","SCANNING SEARCH SPACE\n","\n","216 configurations will be evaluated.\n","1   - conf_ID: EoZPXDti   ****  --  train_acc_A = 0.761,  val_acc_A = 0.762    --  train_acc_B = 0.758,  val_acc_B = 0.771   \n","2   - conf_ID: mKramAEq   ****  --  train_acc_A = 0.754,  val_acc_A = 0.762    --  train_acc_B = 0.767,  val_acc_B = 0.769   \n","3   - conf_ID: wiHBUVLB   ****  --  train_acc_A = 0.728,  val_acc_A = 0.756    --  train_acc_B = 0.765,  val_acc_B = 0.764   \n","4   - conf_ID: uponZUmR   ****  --  train_acc_A = 0.745,  val_acc_A = 0.757    --  train_acc_B = 0.772,  val_acc_B = 0.767   \n","5   - conf_ID: rKTpphsx   ****  --  train_acc_A = 0.75,   val_acc_A = 0.76     --  train_acc_B = 0.754,  val_acc_B = 0.769   \n","6   - conf_ID: XcHWGfWG   ****  --  train_acc_A = 0.793,  val_acc_A = 0.757    --  train_acc_B = 0.773,  val_acc_B = 0.763   \n","7   - conf_ID: phgNeJPE   ****  --  train_acc_A = 0.823,  val_acc_A = 0.756    --  train_acc_B = 0.794,  val_acc_B = 0.761   \n","8   - conf_ID: jLZSHbCB   ****  --  train_acc_A = 0.891,  val_acc_A = 0.765    --  train_acc_B = 0.851,  val_acc_B = 0.774   \n","9   - conf_ID: WRTLsUxL   ****  --  train_acc_A = 0.823,  val_acc_A = 0.755    --  train_acc_B = 0.837,  val_acc_B = 0.76    \n","10  - conf_ID: ndqzWqff   ****  --  train_acc_A = 0.964,  val_acc_A = 0.751    --  train_acc_B = 0.92,   val_acc_B = 0.77    \n","11  - conf_ID: yIaqfHzB   ****  --  train_acc_A = 0.794,  val_acc_A = 0.75     --  train_acc_B = 0.789,  val_acc_B = 0.769   \n","12  - conf_ID: cqAUoRUB   ****  --  train_acc_A = 0.787,  val_acc_A = 0.752    --  train_acc_B = 0.717,  val_acc_B = 0.763   \n","13  - conf_ID: DBkkQHoa   ****  --  train_acc_A = 0.861,  val_acc_A = 0.756    --  train_acc_B = 0.804,  val_acc_B = 0.769   \n","14  - conf_ID: IuGdHJuX   ****  --  train_acc_A = 0.749,  val_acc_A = 0.755    --  train_acc_B = 0.783,  val_acc_B = 0.766   \n","15  - conf_ID: OtKAUCzo   ****  --  train_acc_A = 0.776,  val_acc_A = 0.76     --  train_acc_B = 0.792,  val_acc_B = 0.777   \n","16  - conf_ID: xpHTSQMP   ****  --  train_acc_A = 0.82,   val_acc_A = 0.757    --  train_acc_B = 0.803,  val_acc_B = 0.768   \n","17  - conf_ID: lPhXFeew   ****  --  train_acc_A = 0.815,  val_acc_A = 0.753    --  train_acc_B = 0.801,  val_acc_B = 0.756   \n","18  - conf_ID: ibhVmNYb   ****  --  train_acc_A = 0.775,  val_acc_A = 0.751    --  train_acc_B = 0.769,  val_acc_B = 0.757   \n","19  - conf_ID: HqWtIiPO   ****  --  train_acc_A = 0.741,  val_acc_A = 0.771    --  train_acc_B = 0.737,  val_acc_B = 0.778   \n","20  - conf_ID: NrKVPGYC   ****  --  train_acc_A = 0.733,  val_acc_A = 0.759    --  train_acc_B = 0.734,  val_acc_B = 0.77    \n","21  - conf_ID: gZcMGhYV   ****  --  train_acc_A = 0.715,  val_acc_A = 0.768    --  train_acc_B = 0.739,  val_acc_B = 0.777   \n","22  - conf_ID: MfzDokGX   ****  --  train_acc_A = 0.775,  val_acc_A = 0.771    --  train_acc_B = 0.764,  val_acc_B = 0.777   \n","23  - conf_ID: ssjMJexp   ****  --  train_acc_A = 0.786,  val_acc_A = 0.765    --  train_acc_B = 0.77,   val_acc_B = 0.778   \n","24  - conf_ID: VJJYVBcj   ****  --  train_acc_A = 0.782,  val_acc_A = 0.769    --  train_acc_B = 0.767,  val_acc_B = 0.775   \n","25  - conf_ID: mccJLyjo   ****  --  train_acc_A = 0.789,  val_acc_A = 0.763    --  train_acc_B = 0.756,  val_acc_B = 0.768   \n","26  - conf_ID: LAXTLCKF   ****  --  train_acc_A = 0.771,  val_acc_A = 0.768    --  train_acc_B = 0.732,  val_acc_B = 0.774   \n","27  - conf_ID: YApxrGfq   ****  --  train_acc_A = 0.727,  val_acc_A = 0.751    --  train_acc_B = 0.724,  val_acc_B = 0.755   \n","28  - conf_ID: nksaqIiy   ****  --  train_acc_A = 0.764,  val_acc_A = 0.761    --  train_acc_B = 0.802,  val_acc_B = 0.774   \n","29  - conf_ID: tUVwqrIr   ****  --  train_acc_A = 0.807,  val_acc_A = 0.76     --  train_acc_B = 0.797,  val_acc_B = 0.772   \n","30  - conf_ID: DwQeXswL   ****  --  train_acc_A = 0.739,  val_acc_A = 0.761    --  train_acc_B = 0.767,  val_acc_B = 0.776   \n","31  - conf_ID: TgKDEfFc   ****  --  train_acc_A = 0.748,  val_acc_A = 0.768    --  train_acc_B = 0.783,  val_acc_B = 0.78    \n","32  - conf_ID: dwlzdhbL   ****  --  train_acc_A = 0.772,  val_acc_A = 0.764    --  train_acc_B = 0.772,  val_acc_B = 0.769   \n","33  - conf_ID: BWOUCwYZ   ****  --  train_acc_A = 0.772,  val_acc_A = 0.768    --  train_acc_B = 0.762,  val_acc_B = 0.778   \n","34  - conf_ID: WkEsWAMD   ****  --  train_acc_A = 0.739,  val_acc_A = 0.755    --  train_acc_B = 0.726,  val_acc_B = 0.761   \n","35  - conf_ID: ngERixCO   ****  --  train_acc_A = 0.706,  val_acc_A = 0.754    --  train_acc_B = 0.702,  val_acc_B = 0.758   \n","36  - conf_ID: yKCangqk   ****  --  train_acc_A = 0.674,  val_acc_A = 0.724    --  train_acc_B = 0.667,  val_acc_B = 0.727   \n","37  - conf_ID: TwmXSNnE   ****  --  train_acc_A = 0.774,  val_acc_A = 0.76     --  train_acc_B = 0.77,   val_acc_B = 0.767   \n","38  - conf_ID: PPsmUgHR   ****  --  train_acc_A = 0.768,  val_acc_A = 0.76     --  train_acc_B = 0.764,  val_acc_B = 0.769   \n","39  - conf_ID: xwRtTdHl   ****  --  train_acc_A = 0.747,  val_acc_A = 0.759    --  train_acc_B = 0.736,  val_acc_B = 0.77    \n","40  - conf_ID: eTTRdoQz   ****  --  train_acc_A = 0.746,  val_acc_A = 0.759    --  train_acc_B = 0.746,  val_acc_B = 0.766   \n","41  - conf_ID: XJuNfeYp   ****  --  train_acc_A = 0.793,  val_acc_A = 0.76     --  train_acc_B = 0.784,  val_acc_B = 0.767   \n","42  - conf_ID: mhEPnmRm   ****  --  train_acc_A = 0.768,  val_acc_A = 0.759    --  train_acc_B = 0.77,   val_acc_B = 0.77    \n","43  - conf_ID: AYwAcvSM   ****  --  train_acc_A = 0.809,  val_acc_A = 0.758    --  train_acc_B = 0.774,  val_acc_B = 0.764   \n","44  - conf_ID: JvgFLIlD   ****  --  train_acc_A = 0.839,  val_acc_A = 0.764    --  train_acc_B = 0.799,  val_acc_B = 0.769   \n","45  - conf_ID: SBEwGtkG   ****  --  train_acc_A = 0.799,  val_acc_A = 0.76     --  train_acc_B = 0.822,  val_acc_B = 0.767   \n","46  - conf_ID: vFMsSwfv   ****  --  train_acc_A = 0.799,  val_acc_A = 0.761    --  train_acc_B = 0.829,  val_acc_B = 0.779   \n","47  - conf_ID: JDLgXQYx   ****  --  train_acc_A = 0.839,  val_acc_A = 0.757    --  train_acc_B = 0.768,  val_acc_B = 0.775   \n","48  - conf_ID: gzTBnDSE   ****  --  train_acc_A = 0.764,  val_acc_A = 0.756    --  train_acc_B = 0.774,  val_acc_B = 0.773   \n","49  - conf_ID: yJwlFAhT   ****  --  train_acc_A = 0.785,  val_acc_A = 0.757    --  train_acc_B = 0.838,  val_acc_B = 0.773   \n","50  - conf_ID: dMSHeckc   ****  --  train_acc_A = 0.823,  val_acc_A = 0.76     --  train_acc_B = 0.798,  val_acc_B = 0.767   \n","51  - conf_ID: CLQOuNso   ****  --  train_acc_A = 0.818,  val_acc_A = 0.761    --  train_acc_B = 0.797,  val_acc_B = 0.774   \n","52  - conf_ID: uJjnCDYQ   ****  --  train_acc_A = 0.82,   val_acc_A = 0.757    --  train_acc_B = 0.847,  val_acc_B = 0.761   \n","53  - conf_ID: rgLuzAGk   ****  --  train_acc_A = 0.787,  val_acc_A = 0.756    --  train_acc_B = 0.794,  val_acc_B = 0.762   \n","54  - conf_ID: eePAQhaW   ****  --  train_acc_A = 0.768,  val_acc_A = 0.745    --  train_acc_B = 0.774,  val_acc_B = 0.75    \n","55  - conf_ID: jYDRGShm   ****  --  train_acc_A = 0.739,  val_acc_A = 0.765    --  train_acc_B = 0.729,  val_acc_B = 0.776   \n","56  - conf_ID: PiJyGHtA   ****  --  train_acc_A = 0.762,  val_acc_A = 0.765    --  train_acc_B = 0.733,  val_acc_B = 0.772   \n","57  - conf_ID: xecskJKB   ****  --  train_acc_A = 0.723,  val_acc_A = 0.771    --  train_acc_B = 0.721,  val_acc_B = 0.78    \n","58  - conf_ID: uRamRiNQ   ****  --  train_acc_A = 0.748,  val_acc_A = 0.772    --  train_acc_B = 0.762,  val_acc_B = 0.779   \n","59  - conf_ID: sQqJxQbw   ****  --  train_acc_A = 0.777,  val_acc_A = 0.77     --  train_acc_B = 0.745,  val_acc_B = 0.78    \n","60  - conf_ID: eJfuOaRB   ****  --  train_acc_A = 0.764,  val_acc_A = 0.763    --  train_acc_B = 0.741,  val_acc_B = 0.772   \n","61  - conf_ID: dZwuFghF   ****  --  train_acc_A = 0.758,  val_acc_A = 0.772    --  train_acc_B = 0.756,  val_acc_B = 0.775   \n","62  - conf_ID: iRZNPYmu   ****  --  train_acc_A = 0.749,  val_acc_A = 0.755    --  train_acc_B = 0.745,  val_acc_B = 0.759   \n","63  - conf_ID: TawWrveW   ****  --  train_acc_A = 0.72,   val_acc_A = 0.758    --  train_acc_B = 0.717,  val_acc_B = 0.765   \n","64  - conf_ID: TdfwAUmU   ****  --  train_acc_A = 0.733,  val_acc_A = 0.771    --  train_acc_B = 0.76,   val_acc_B = 0.777   \n","65  - conf_ID: EmVhIKpt   ****  --  train_acc_A = 0.765,  val_acc_A = 0.769    --  train_acc_B = 0.75,   val_acc_B = 0.778   \n","66  - conf_ID: eqjdztym   ****  --  train_acc_A = 0.745,  val_acc_A = 0.766    --  train_acc_B = 0.757,  val_acc_B = 0.773   \n","67  - conf_ID: ZbLArlgh   ****  --  train_acc_A = 0.757,  val_acc_A = 0.769    --  train_acc_B = 0.762,  val_acc_B = 0.78    \n","68  - conf_ID: OtPjjEBk   ****  --  train_acc_A = 0.77,   val_acc_A = 0.761    --  train_acc_B = 0.755,  val_acc_B = 0.776   \n","69  - conf_ID: DsRYHYKV   ****  --  train_acc_A = 0.744,  val_acc_A = 0.76     --  train_acc_B = 0.742,  val_acc_B = 0.768   \n","70  - conf_ID: WaIXviyV   ****  --  train_acc_A = 0.72,   val_acc_A = 0.76     --  train_acc_B = 0.719,  val_acc_B = 0.763   \n","71  - conf_ID: pRBgluzb   ****  --  train_acc_A = 0.696,  val_acc_A = 0.749    --  train_acc_B = 0.692,  val_acc_B = 0.753   \n","72  - conf_ID: XZGJYrPU   ****  --  train_acc_A = 0.666,  val_acc_A = 0.735    --  train_acc_B = 0.663,  val_acc_B = 0.738   \n","73  - conf_ID: JUSFVmKs   ****  --  train_acc_A = 0.778,  val_acc_A = 0.759    --  train_acc_B = 0.744,  val_acc_B = 0.765   \n","74  - conf_ID: HqVHzkgr   ****  --  train_acc_A = 0.769,  val_acc_A = 0.757    --  train_acc_B = 0.747,  val_acc_B = 0.767   \n","75  - conf_ID: jTIAXVuZ   ****  --  train_acc_A = 0.754,  val_acc_A = 0.757    --  train_acc_B = 0.753,  val_acc_B = 0.764   \n","76  - conf_ID: UlsBWFcA   ****  --  train_acc_A = 0.761,  val_acc_A = 0.762    --  train_acc_B = 0.767,  val_acc_B = 0.772   \n","77  - conf_ID: BGnrDLWn   ****  --  train_acc_A = 0.751,  val_acc_A = 0.761    --  train_acc_B = 0.752,  val_acc_B = 0.768   \n","78  - conf_ID: FzlFEGAZ   ****  --  train_acc_A = 0.788,  val_acc_A = 0.759    --  train_acc_B = 0.789,  val_acc_B = 0.768   \n","79  - conf_ID: ClATPhPc   ****  --  train_acc_A = 0.813,  val_acc_A = 0.755    --  train_acc_B = 0.786,  val_acc_B = 0.762   \n","80  - conf_ID: sEiKPTMh   ****  --  train_acc_A = 0.806,  val_acc_A = 0.756    --  train_acc_B = 0.815,  val_acc_B = 0.763   \n","81  - conf_ID: GimmSbcG   ****  --  train_acc_A = 0.857,  val_acc_A = 0.761    --  train_acc_B = 0.813,  val_acc_B = 0.767   \n","82  - conf_ID: fQoQmCVJ   ****  --  train_acc_A = 0.79,   val_acc_A = 0.757    --  train_acc_B = 0.854,  val_acc_B = 0.77    \n","83  - conf_ID: ORSwjogg   ****  --  train_acc_A = 0.855,  val_acc_A = 0.755    --  train_acc_B = 0.813,  val_acc_B = 0.773   \n","84  - conf_ID: nzCdEzln   ****  --  train_acc_A = 0.836,  val_acc_A = 0.761    --  train_acc_B = 0.778,  val_acc_B = 0.777   \n","85  - conf_ID: jpaeAfiX   ****  --  train_acc_A = 0.846,  val_acc_A = 0.761    --  train_acc_B = 0.835,  val_acc_B = 0.775   \n","86  - conf_ID: BNSKzhfn   ****  --  train_acc_A = 0.768,  val_acc_A = 0.753    --  train_acc_B = 0.816,  val_acc_B = 0.767   \n","87  - conf_ID: asbmjCWq   ****  --  train_acc_A = 0.866,  val_acc_A = 0.755    --  train_acc_B = 0.843,  val_acc_B = 0.77    \n","88  - conf_ID: QftPPEIR   ****  --  train_acc_A = 0.82,   val_acc_A = 0.758    --  train_acc_B = 0.825,  val_acc_B = 0.768   \n","89  - conf_ID: EFqcwUGT   ****  --  train_acc_A = 0.803,  val_acc_A = 0.758    --  train_acc_B = 0.81,   val_acc_B = 0.768   \n","90  - conf_ID: YsNuIdfR   ****  --  train_acc_A = 0.777,  val_acc_A = 0.755    --  train_acc_B = 0.767,  val_acc_B = 0.759   \n","91  - conf_ID: QUICSWkS   ****  --  train_acc_A = 0.747,  val_acc_A = 0.765    --  train_acc_B = 0.731,  val_acc_B = 0.775   \n","92  - conf_ID: GaQJkspT   ****  --  train_acc_A = 0.738,  val_acc_A = 0.765    --  train_acc_B = 0.727,  val_acc_B = 0.775   \n","93  - conf_ID: EFgUeyDp   ****  --  train_acc_A = 0.72,   val_acc_A = 0.755    --  train_acc_B = 0.714,  val_acc_B = 0.767   \n","94  - conf_ID: oOnNsSFO   ****  --  train_acc_A = 0.762,  val_acc_A = 0.77     --  train_acc_B = 0.778,  val_acc_B = 0.777   \n","95  - conf_ID: DtEoliZN   ****  --  train_acc_A = 0.77,   val_acc_A = 0.771    --  train_acc_B = 0.747,  val_acc_B = 0.775   \n","96  - conf_ID: irKKmpaz   ****  --  train_acc_A = 0.756,  val_acc_A = 0.767    --  train_acc_B = 0.748,  val_acc_B = 0.775   \n","97  - conf_ID: byCFRDeN   ****  --  train_acc_A = 0.755,  val_acc_A = 0.771    --  train_acc_B = 0.745,  val_acc_B = 0.776   \n","98  - conf_ID: VePmPpwT   ****  --  train_acc_A = 0.746,  val_acc_A = 0.765    --  train_acc_B = 0.727,  val_acc_B = 0.768   \n","99  - conf_ID: SWIMxEsL   ****  --  train_acc_A = 0.698,  val_acc_A = 0.751    --  train_acc_B = 0.696,  val_acc_B = 0.753   \n","100 - conf_ID: IQcJfZVY   ****  --  train_acc_A = 0.854,  val_acc_A = 0.77     --  train_acc_B = 0.778,  val_acc_B = 0.783   \n","101 - conf_ID: khGcbSnl   ****  --  train_acc_A = 0.813,  val_acc_A = 0.768    --  train_acc_B = 0.763,  val_acc_B = 0.781   \n","102 - conf_ID: TGMcDusZ   ****  --  train_acc_A = 0.739,  val_acc_A = 0.769    --  train_acc_B = 0.731,  val_acc_B = 0.775   \n","103 - conf_ID: mcIaYmSW   ****  --  train_acc_A = 0.734,  val_acc_A = 0.768    --  train_acc_B = 0.759,  val_acc_B = 0.779   \n","104 - conf_ID: DxjTYxcc   ****  --  train_acc_A = 0.742,  val_acc_A = 0.769    --  train_acc_B = 0.775,  val_acc_B = 0.778   \n","105 - conf_ID: NCifRFPX   ****  --  train_acc_A = 0.743,  val_acc_A = 0.772    --  train_acc_B = 0.737,  val_acc_B = 0.78    \n","106 - conf_ID: zdfhSomj   ****  --  train_acc_A = 0.708,  val_acc_A = 0.751    --  train_acc_B = 0.705,  val_acc_B = 0.758   \n","107 - conf_ID: YiBBebct   ****  --  train_acc_A = 0.678,  val_acc_A = 0.739    --  train_acc_B = 0.68,   val_acc_B = 0.741   \n","108 - conf_ID: QMKwCvJU   ****  --  train_acc_A = 0.66,   val_acc_A = 0.725    --  train_acc_B = 0.653,  val_acc_B = 0.728   \n","109 - conf_ID: NMcEIlsQ   ****  --  train_acc_A = 0.78,   val_acc_A = 0.754    --  train_acc_B = 0.746,  val_acc_B = 0.763   \n","110 - conf_ID: MZRPGJYH   ****  --  train_acc_A = 0.746,  val_acc_A = 0.76     --  train_acc_B = 0.738,  val_acc_B = 0.768   \n","111 - conf_ID: iZbCVNSi   ****  --  train_acc_A = 0.762,  val_acc_A = 0.76     --  train_acc_B = 0.764,  val_acc_B = 0.769   \n","112 - conf_ID: GkttahRO   ****  --  train_acc_A = 0.787,  val_acc_A = 0.76     --  train_acc_B = 0.782,  val_acc_B = 0.765   \n","113 - conf_ID: HsuyCoCF   ****  --  train_acc_A = 0.752,  val_acc_A = 0.759    --  train_acc_B = 0.775,  val_acc_B = 0.768   \n","114 - conf_ID: WYAQiEvG   ****  --  train_acc_A = 0.776,  val_acc_A = 0.762    --  train_acc_B = 0.792,  val_acc_B = 0.766   \n","115 - conf_ID: fCpeSOAA   ****  --  train_acc_A = 0.854,  val_acc_A = 0.755    --  train_acc_B = 0.845,  val_acc_B = 0.76    \n","116 - conf_ID: hfQWySdd   ****  --  train_acc_A = 0.838,  val_acc_A = 0.75     --  train_acc_B = 0.815,  val_acc_B = 0.754   \n","117 - conf_ID: KIMJLzby   ****  --  train_acc_A = 0.81,   val_acc_A = 0.764    --  train_acc_B = 0.819,  val_acc_B = 0.769   \n","118 - conf_ID: YXWJWrWU   ****  --  train_acc_A = 0.718,  val_acc_A = 0.742    --  train_acc_B = 0.809,  val_acc_B = 0.765   \n","119 - conf_ID: QsLACEtS   ****  --  train_acc_A = 0.787,  val_acc_A = 0.764    --  train_acc_B = 0.798,  val_acc_B = 0.774   \n","120 - conf_ID: NIzHXRCT   ****  --  train_acc_A = 0.818,  val_acc_A = 0.74     --  train_acc_B = 0.716,  val_acc_B = 0.76    \n","121 - conf_ID: eJnpzjhz   ****  --  train_acc_A = 0.789,  val_acc_A = 0.762    --  train_acc_B = 0.786,  val_acc_B = 0.764   \n","122 - conf_ID: abfPhaRT   ****  --  train_acc_A = 0.792,  val_acc_A = 0.757    --  train_acc_B = 0.819,  val_acc_B = 0.765   \n","123 - conf_ID: XgAkvfzk   ****  --  train_acc_A = 0.777,  val_acc_A = 0.748    --  train_acc_B = 0.777,  val_acc_B = 0.76    \n","124 - conf_ID: DuxywSwb   ****  --  train_acc_A = 0.84,   val_acc_A = 0.75     --  train_acc_B = 0.82,   val_acc_B = 0.754   \n","125 - conf_ID: VKkMLiGl   ****  --  train_acc_A = 0.799,  val_acc_A = 0.75     --  train_acc_B = 0.782,  val_acc_B = 0.755   \n","126 - conf_ID: nSrqjhtw   ****  --  train_acc_A = 0.745,  val_acc_A = 0.74     --  train_acc_B = 0.757,  val_acc_B = 0.747   \n","127 - conf_ID: ILqqcuJq   ****  --  train_acc_A = 0.709,  val_acc_A = 0.757    --  train_acc_B = 0.712,  val_acc_B = 0.765   \n","128 - conf_ID: QSzBFpFn   ****  --  train_acc_A = 0.74,   val_acc_A = 0.756    --  train_acc_B = 0.723,  val_acc_B = 0.77    \n","129 - conf_ID: uOBsIdNV   ****  --  train_acc_A = 0.698,  val_acc_A = 0.763    --  train_acc_B = 0.706,  val_acc_B = 0.772   \n","130 - conf_ID: xjbCJEMi   ****  --  train_acc_A = 0.76,   val_acc_A = 0.759    --  train_acc_B = 0.745,  val_acc_B = 0.769   \n","131 - conf_ID: cYDWcEQl   ****  --  train_acc_A = 0.753,  val_acc_A = 0.763    --  train_acc_B = 0.75,   val_acc_B = 0.771   \n","132 - conf_ID: BimSnMEd   ****  --  train_acc_A = 0.754,  val_acc_A = 0.76     --  train_acc_B = 0.756,  val_acc_B = 0.768   \n","133 - conf_ID: QxSgluHp   ****  --  train_acc_A = 0.751,  val_acc_A = 0.752    --  train_acc_B = 0.746,  val_acc_B = 0.755   \n","134 - conf_ID: zIbIRHCM   ****  --  train_acc_A = 0.735,  val_acc_A = 0.764    --  train_acc_B = 0.734,  val_acc_B = 0.765   \n","135 - conf_ID: BWpuwbgF   ****  --  train_acc_A = 0.722,  val_acc_A = 0.75     --  train_acc_B = 0.718,  val_acc_B = 0.755   \n","136 - conf_ID: LkVtAzUm   ****  --  train_acc_A = 0.723,  val_acc_A = 0.763    --  train_acc_B = 0.766,  val_acc_B = 0.772   \n","137 - conf_ID: KgZUKVHr   ****  --  train_acc_A = 0.829,  val_acc_A = 0.755    --  train_acc_B = 0.778,  val_acc_B = 0.765   \n","138 - conf_ID: OKxiCmAE   ****  --  train_acc_A = 0.763,  val_acc_A = 0.756    --  train_acc_B = 0.783,  val_acc_B = 0.774   \n","139 - conf_ID: KvxnNVeH   ****  --  train_acc_A = 0.783,  val_acc_A = 0.764    --  train_acc_B = 0.82,   val_acc_B = 0.774   \n","140 - conf_ID: ufGFaSXF   ****  --  train_acc_A = 0.759,  val_acc_A = 0.76     --  train_acc_B = 0.769,  val_acc_B = 0.77    \n","141 - conf_ID: udwAWQwj   ****  --  train_acc_A = 0.776,  val_acc_A = 0.757    --  train_acc_B = 0.734,  val_acc_B = 0.767   \n","142 - conf_ID: wfrJlbkF   ****  --  train_acc_A = 0.717,  val_acc_A = 0.746    --  train_acc_B = 0.716,  val_acc_B = 0.751   \n","143 - conf_ID: jSOfrQDs   ****  --  train_acc_A = 0.692,  val_acc_A = 0.739    --  train_acc_B = 0.696,  val_acc_B = 0.741   \n","144 - conf_ID: tEhMeSLr   ****  --  train_acc_A = 0.658,  val_acc_A = 0.725    --  train_acc_B = 0.66,   val_acc_B = 0.728   \n","145 - conf_ID: inhuTAeP   ****  --  train_acc_A = 0.755,  val_acc_A = 0.762    --  train_acc_B = 0.748,  val_acc_B = 0.769   \n","146 - conf_ID: KCiqqmPc   ****  --  train_acc_A = 0.787,  val_acc_A = 0.761    --  train_acc_B = 0.786,  val_acc_B = 0.769   \n","147 - conf_ID: ZOoEIzZb   ****  --  train_acc_A = 0.749,  val_acc_A = 0.764    --  train_acc_B = 0.769,  val_acc_B = 0.773   \n","148 - conf_ID: sESvShCw   ****  --  train_acc_A = 0.778,  val_acc_A = 0.754    --  train_acc_B = 0.768,  val_acc_B = 0.763   \n","149 - conf_ID: AKlXeRMc   ****  --  train_acc_A = 0.767,  val_acc_A = 0.756    --  train_acc_B = 0.791,  val_acc_B = 0.764   \n","150 - conf_ID: mrZvLfFg   ****  --  train_acc_A = 0.754,  val_acc_A = 0.764    --  train_acc_B = 0.773,  val_acc_B = 0.77    \n","151 - conf_ID: BnPRisdm   ****  --  train_acc_A = 0.853,  val_acc_A = 0.76     --  train_acc_B = 0.803,  val_acc_B = 0.768   \n","152 - conf_ID: GXnbUmTD   ****  --  train_acc_A = 0.81,   val_acc_A = 0.757    --  train_acc_B = 0.81,   val_acc_B = 0.764   \n","153 - conf_ID: cDKoJkrE   ****  --  train_acc_A = 0.764,  val_acc_A = 0.75     --  train_acc_B = 0.808,  val_acc_B = 0.757   \n","154 - conf_ID: xmkafhJQ   ****  --  train_acc_A = 0.766,  val_acc_A = 0.755    --  train_acc_B = 0.817,  val_acc_B = 0.769   \n","155 - conf_ID: VydBGsvy   ****  --  train_acc_A = 0.857,  val_acc_A = 0.746    --  train_acc_B = 0.786,  val_acc_B = 0.765   \n","156 - conf_ID: OJNMVKpV   ****  --  train_acc_A = 0.835,  val_acc_A = 0.749    --  train_acc_B = 0.731,  val_acc_B = 0.764   \n","157 - conf_ID: cZiVXQGE   ****  --  train_acc_A = 0.804,  val_acc_A = 0.757    --  train_acc_B = 0.781,  val_acc_B = 0.762   \n","158 - conf_ID: LKgrkGbl   ****  --  train_acc_A = 0.812,  val_acc_A = 0.754    --  train_acc_B = 0.792,  val_acc_B = 0.771   \n","159 - conf_ID: pIryMYRS   ****  --  train_acc_A = 0.835,  val_acc_A = 0.751    --  train_acc_B = 0.812,  val_acc_B = 0.766   \n","160 - conf_ID: hPiAajKl   ****  --  train_acc_A = 0.825,  val_acc_A = 0.757    --  train_acc_B = 0.817,  val_acc_B = 0.764   \n","161 - conf_ID: fVQOJObI   ****  --  train_acc_A = 0.797,  val_acc_A = 0.753    --  train_acc_B = 0.76,   val_acc_B = 0.761   \n","162 - conf_ID: XKSQXtLX   ****  --  train_acc_A = 0.758,  val_acc_A = 0.742    --  train_acc_B = 0.752,  val_acc_B = 0.749   \n","163 - conf_ID: RAngKndZ   ****  --  train_acc_A = 0.764,  val_acc_A = 0.755    --  train_acc_B = 0.739,  val_acc_B = 0.765   \n","164 - conf_ID: eEDOswvE   ****  --  train_acc_A = 0.714,  val_acc_A = 0.762    --  train_acc_B = 0.727,  val_acc_B = 0.773   \n","165 - conf_ID: mcdPrpdn   ****  --  train_acc_A = 0.721,  val_acc_A = 0.764    --  train_acc_B = 0.735,  val_acc_B = 0.774   \n","166 - conf_ID: yZeAiBCV   ****  --  train_acc_A = 0.763,  val_acc_A = 0.772    --  train_acc_B = 0.759,  val_acc_B = 0.782   \n","167 - conf_ID: AiAzkaRs   ****  --  train_acc_A = 0.769,  val_acc_A = 0.764    --  train_acc_B = 0.755,  val_acc_B = 0.773   \n","168 - conf_ID: ZFinWIko   ****  --  train_acc_A = 0.743,  val_acc_A = 0.763    --  train_acc_B = 0.731,  val_acc_B = 0.771   \n","169 - conf_ID: zWvEihfu   ****  --  train_acc_A = 0.746,  val_acc_A = 0.758    --  train_acc_B = 0.74,   val_acc_B = 0.761   \n","170 - conf_ID: wynbMuIe   ****  --  train_acc_A = 0.733,  val_acc_A = 0.755    --  train_acc_B = 0.726,  val_acc_B = 0.759   \n","171 - conf_ID: fwgrYrsd   ****  --  train_acc_A = 0.7,    val_acc_A = 0.747    --  train_acc_B = 0.703,  val_acc_B = 0.749   \n","172 - conf_ID: gHoucBWW   ****  --  train_acc_A = 0.762,  val_acc_A = 0.767    --  train_acc_B = 0.77,   val_acc_B = 0.778   \n","173 - conf_ID: LkxBMRxW   ****  --  train_acc_A = 0.769,  val_acc_A = 0.762    --  train_acc_B = 0.771,  val_acc_B = 0.774   \n","174 - conf_ID: jOIsqCor   ****  --  train_acc_A = 0.766,  val_acc_A = 0.761    --  train_acc_B = 0.76,   val_acc_B = 0.77    \n","175 - conf_ID: yuAWdmyI   ****  --  train_acc_A = 0.762,  val_acc_A = 0.766    --  train_acc_B = 0.754,  val_acc_B = 0.775   \n","176 - conf_ID: VmmZSIVt   ****  --  train_acc_A = 0.765,  val_acc_A = 0.76     --  train_acc_B = 0.799,  val_acc_B = 0.771   \n","177 - conf_ID: NwkVOXEQ   ****  --  train_acc_A = 0.757,  val_acc_A = 0.756    --  train_acc_B = 0.75,   val_acc_B = 0.765   \n","178 - conf_ID: vJfZmpqN   ****  --  train_acc_A = 0.712,  val_acc_A = 0.753    --  train_acc_B = 0.706,  val_acc_B = 0.755   \n","179 - conf_ID: WscYogjg   ****  --  train_acc_A = 0.685,  val_acc_A = 0.73     --  train_acc_B = 0.673,  val_acc_B = 0.735   \n","180 - conf_ID: GnMvAOzl   ****  --  train_acc_A = 0.647,  val_acc_A = 0.708    --  train_acc_B = 0.649,  val_acc_B = 0.711   \n","181 - conf_ID: IrabFiOG   ****  --  train_acc_A = 0.758,  val_acc_A = 0.76     --  train_acc_B = 0.8,    val_acc_B = 0.768   \n","182 - conf_ID: KTDngwRY   ****  --  train_acc_A = 0.78,   val_acc_A = 0.76     --  train_acc_B = 0.77,   val_acc_B = 0.769   \n","183 - conf_ID: OKwPgTAa   ****  --  train_acc_A = 0.762,  val_acc_A = 0.758    --  train_acc_B = 0.765,  val_acc_B = 0.765   \n","184 - conf_ID: QKvtJxKX   ****  --  train_acc_A = 0.795,  val_acc_A = 0.761    --  train_acc_B = 0.774,  val_acc_B = 0.769   \n","185 - conf_ID: XXBsFaji   ****  --  train_acc_A = 0.791,  val_acc_A = 0.757    --  train_acc_B = 0.766,  val_acc_B = 0.765   \n","186 - conf_ID: PRyKavHp   ****  --  train_acc_A = 0.787,  val_acc_A = 0.762    --  train_acc_B = 0.764,  val_acc_B = 0.769   \n","187 - conf_ID: oNYTPCgk   ****  --  train_acc_A = 0.835,  val_acc_A = 0.758    --  train_acc_B = 0.821,  val_acc_B = 0.763   \n","188 - conf_ID: ZEYUraOv   ****  --  train_acc_A = 0.787,  val_acc_A = 0.755    --  train_acc_B = 0.807,  val_acc_B = 0.76    \n","189 - conf_ID: QBavBLOS   ****  --  train_acc_A = 0.836,  val_acc_A = 0.754    --  train_acc_B = 0.802,  val_acc_B = 0.765   \n","190 - conf_ID: kThYiHrw   ****  --  train_acc_A = 0.889,  val_acc_A = 0.754    --  train_acc_B = 0.831,  val_acc_B = 0.76    \n","191 - conf_ID: VSLqkFLV   ****  --  train_acc_A = 0.811,  val_acc_A = 0.757    --  train_acc_B = 0.819,  val_acc_B = 0.768   \n","192 - conf_ID: ilhXGsMF   ****  --  train_acc_A = 0.802,  val_acc_A = 0.74     --  train_acc_B = 0.77,   val_acc_B = 0.763   \n","193 - conf_ID: UMNtDxXq   ****  --  train_acc_A = 0.81,   val_acc_A = 0.751    --  train_acc_B = 0.798,  val_acc_B = 0.765   \n","194 - conf_ID: BHnyTJXj   ****  --  train_acc_A = 0.849,  val_acc_A = 0.753    --  train_acc_B = 0.83,   val_acc_B = 0.765   \n","195 - conf_ID: PJAPyzGZ   ****  --  train_acc_A = 0.742,  val_acc_A = 0.751    --  train_acc_B = 0.792,  val_acc_B = 0.765   \n","196 - conf_ID: ttmVBwsi   ****  --  train_acc_A = 0.779,  val_acc_A = 0.758    --  train_acc_B = 0.798,  val_acc_B = 0.765   \n","197 - conf_ID: FYIDTcgM   ****  --  train_acc_A = 0.803,  val_acc_A = 0.76     --  train_acc_B = 0.789,  val_acc_B = 0.766   \n","198 - conf_ID: pFwrWXyZ   ****  --  train_acc_A = 0.763,  val_acc_A = 0.743    --  train_acc_B = 0.75,   val_acc_B = 0.749   \n","199 - conf_ID: HUdwmKFq   ****  --  train_acc_A = 0.738,  val_acc_A = 0.76     --  train_acc_B = 0.721,  val_acc_B = 0.768   \n","200 - conf_ID: OCzoQzWF   ****  --  train_acc_A = 0.744,  val_acc_A = 0.766    --  train_acc_B = 0.706,  val_acc_B = 0.772   \n","201 - conf_ID: rQhqhcaE   ****  --  train_acc_A = 0.731,  val_acc_A = 0.761    --  train_acc_B = 0.72,   val_acc_B = 0.769   \n","202 - conf_ID: tATpdOWi   ****  --  train_acc_A = 0.803,  val_acc_A = 0.768    --  train_acc_B = 0.769,  val_acc_B = 0.774   \n","203 - conf_ID: KgnhSfku   ****  --  train_acc_A = 0.755,  val_acc_A = 0.768    --  train_acc_B = 0.736,  val_acc_B = 0.775   \n","204 - conf_ID: DboIwmeF   ****  --  train_acc_A = 0.762,  val_acc_A = 0.766    --  train_acc_B = 0.74,   val_acc_B = 0.771   \n","205 - conf_ID: JNcGYKQz   ****  --  train_acc_A = 0.725,  val_acc_A = 0.751    --  train_acc_B = 0.728,  val_acc_B = 0.754   \n","206 - conf_ID: UaTEtmRr   ****  --  train_acc_A = 0.708,  val_acc_A = 0.751    --  train_acc_B = 0.708,  val_acc_B = 0.755   \n","207 - conf_ID: iqFFDtnp   ****  --  train_acc_A = 0.694,  val_acc_A = 0.736    --  train_acc_B = 0.692,  val_acc_B = 0.74    \n","208 - conf_ID: NAQRGCFB   ****  --  train_acc_A = 0.766,  val_acc_A = 0.765    --  train_acc_B = 0.763,  val_acc_B = 0.774   \n","209 - conf_ID: NVExUjiN   ****  --  train_acc_A = 0.799,  val_acc_A = 0.765    --  train_acc_B = 0.743,  val_acc_B = 0.777   \n","210 - conf_ID: JOscgTgX   ****  --  train_acc_A = 0.778,  val_acc_A = 0.761    --  train_acc_B = 0.753,  val_acc_B = 0.773   \n","211 - conf_ID: ZLoUpCcq   ****  --  train_acc_A = 0.762,  val_acc_A = 0.765    --  train_acc_B = 0.753,  val_acc_B = 0.771   \n","212 - conf_ID: bMbsiLKV   ****  --  train_acc_A = 0.793,  val_acc_A = 0.766    --  train_acc_B = 0.776,  val_acc_B = 0.772   \n","213 - conf_ID: WEJxwtjr   ****  --  train_acc_A = 0.755,  val_acc_A = 0.761    --  train_acc_B = 0.745,  val_acc_B = 0.769   \n","214 - conf_ID: AKAQRbfR   ****  --  train_acc_A = 0.705,  val_acc_A = 0.752    --  train_acc_B = 0.7,    val_acc_B = 0.754   \n","215 - conf_ID: TGqVtgzk   ****  --  train_acc_A = 0.682,  val_acc_A = 0.727    --  train_acc_B = 0.681,  val_acc_B = 0.728   \n","216 - conf_ID: Gonxzvgq   ****  --  train_acc_A = 0.638,  val_acc_A = 0.72     --  train_acc_B = 0.639,  val_acc_B = 0.722   \n","\n","DONE. 216 CONFIGURATIONS WERE SUCCESFULLY EVALUATED.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":147},"id":"F7E5RdlL2VxR","executionInfo":{"status":"ok","timestamp":1616569757536,"user_tz":360,"elapsed":588,"user":{"displayName":"Felipe Ramírez Brindis","photoUrl":"","userId":"16823923198286569615"}},"outputId":"ba213f81-a72b-46f0-c33a-0e62c51a01a6"},"source":["results_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>conf_ID</th>\n","      <th>L1_size</th>\n","      <th>L2_size</th>\n","      <th>activation</th>\n","      <th>p_dropout_1</th>\n","      <th>p_dropout_2</th>\n","      <th>optimizer</th>\n","      <th>batch_size</th>\n","      <th>max_epochs</th>\n","      <th>best_epochh</th>\n","      <th>train_acc_A</th>\n","      <th>val_acc_A</th>\n","      <th>min2best</th>\n","      <th>max2best</th>\n","      <th>train_acc_B</th>\n","      <th>val_acc_B</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>beYZusxb</td>\n","      <td>300</td>\n","      <td>75</td>\n","      <td>relu</td>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","      <td>adam-5e-4</td>\n","      <td>1024</td>\n","      <td>75</td>\n","      <td>69</td>\n","      <td>0.777050</td>\n","      <td>0.765843</td>\n","      <td>42</td>\n","      <td>60</td>\n","      <td>0.739776</td>\n","      <td>0.777043</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GpuPeLFz</td>\n","      <td>200</td>\n","      <td>75</td>\n","      <td>relu</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>rmsprop-5e-4</td>\n","      <td>256</td>\n","      <td>75</td>\n","      <td>30</td>\n","      <td>0.865485</td>\n","      <td>0.759539</td>\n","      <td>15</td>\n","      <td>33</td>\n","      <td>0.826923</td>\n","      <td>0.772138</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    conf_ID  L1_size  L2_size  ... max2best  train_acc_B  val_acc_B\n","0  beYZusxb      300       75  ...       60     0.739776   0.777043\n","1  GpuPeLFz      200       75  ...       33     0.826923   0.772138\n","\n","[2 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"y0KadG6bM3qM"},"source":["# NN TRAINING A GLOBAL CLASSIFIER"]},{"cell_type":"markdown","metadata":{"id":"5m9d9sOgM3qX"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ANG0pfOPM3qZ","executionInfo":{"status":"ok","timestamp":1616485371343,"user_tz":360,"elapsed":1463,"user":{"displayName":"Felipe RB","photoUrl":"","userId":"00787313856272916889"}},"outputId":"d555ca9b-2832-41ef-fcf1-bee0dc6933b7"},"source":["X_train, Y_train = dataset_utils.loadEncodedTrainData(embedding_type='FT3',\n","                                                 encoding_format='SINGLE-VEC',\n","                                                 labels_to_return=['HS'],\n","                                                 n_folds=7)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: SINGLE-VEC\n","\n","Process complete\n","5000 train instances retrieved\n","\n","encodings_dim = (300,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9tykaqR_Az0t"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OeBbRIG4Al1S"},"source":["X_train_merged = np.concatenate([X_train[i] for i in range(7)], axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JpUekAqSA-gd"},"source":["Y_train_labels = Y_train['HS']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LniwSmIxBut7"},"source":["## GETTING THE KERAS MODEL"]},{"cell_type":"code","metadata":{"id":"Kq8radWxBt53"},"source":["arch_params= {\n","    'L1_size': 200,                    \n","    'L2_size': 75,                \n","    'activation':'relu',                                    \n","    'p_dropout_1': 0.5,\n","    'p_dropout_2': 0.5                                \n","}\n","\n","optimizer = keras_optimizers.Adam(learning_rate=0.0005)\n","\n","my_keras_model = fcNeuralNetworkModel('HS', arch_params, optimizer, verbose=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3iPKAv_BCcHb"},"source":["## FITTING THE KERAS MODEL"]},{"cell_type":"code","metadata":{"id":"AFdsJHjuCWxS"},"source":["my_keras_model.fit(x=X_train_merged, \n","            y=Y_train_labels,\n","            epochs=50,\n","            batch_size=512,\n","            verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"biTlP5uUGSo9"},"source":["## TESTING"]},{"cell_type":"code","metadata":{"id":"k2e3Ze4XHHGW"},"source":["def loadEncodedTestData(embedding_type, encoding_format, labels_to_return):\n","  \"\"\" \n","  Loads encoded test dataset from drive.\n","  \n","  Input:\n","  embedding_type   - str, valid_types = ['FT1', 'FT2', 'FT3', 'W2V100', 'W2V300', 'GloVe100', 'GloVe300']\n","  encoding_format  - str, valid_foramts = ['SINGLE-VEC', 'EMB-SEQ']\n","\n","  Output:\n","  (X_test, Y_test) -- tuple containing the encoded dataset\n","\n","  --\n","  X_train   - list[numpy_array], encoded train-set partitioned in K subsets\n","  Y_train   - DataFrame, train set labels\n","  X_test    - list[numpy_array], encoded test-set\n","  Y_tes     - DataFrame, test set labels\n","\n","  \"\"\"\n","  valid_embedding_types = ['FT1', 'FT2', 'FT3', 'W2V100', 'W2V300', 'GloVe100', 'GloVe300']\n","  valid_encoding_format_options = ['SINGLE-VEC', 'EMB-SEQ']\n","\n","  if embedding_type not in valid_embedding_types:\n","    print('Invalid embedding_type option. No data was returned.\\n')\n","    print('...')\n","    print(\"Valid embedding types: ['FT1', 'FT2', 'FT3', 'W2V100', 'W2V300', 'GloVe100', 'GloVe300']\")\n","    return (None, None)\n","\n","  if encoding_format not in valid_encoding_format_options:\n","    print('Invalid format option. No data was returned.\\n')\n","    print('...')\n","    print(\"Valid formats: ['SINGLE-VEC', 'EMB-SEQ']\")\n","    return (None, None)\n","\n","  EMBEDDINGS_INFO = {'FT1':'FastText 1 - Common Crawl + Wikipedia',\n","                    'FT2':'FastText 2 - Esp. Wikipedia',\n","                    'FT3':'FastText 3 - Spanish Unannotated Corpora',\n","                    'W2V300':'W2V 300d - Spanish Unannotated Corpora',\n","                    'W2V100':'W2V 100d - Spanish CoNLL',\n","                    'GloVe300':'GloVe 300d - Spanish Billion Word Corpus',\n","                    'GloVe100':'GloVe 100d - Spanish Billion Word Corpus'}\n","\n","  embedding_info = EMBEDDINGS_INFO[embedding_type]\n","\n","  print(embedding_info)\n","  print('Encoding Format: {}'.format(encoding_format))\n","\n","  import pandas as pd\n","  prep_format = 2\n","\n","  # DATA\n","  file_name = '{}_TEST_P{}.data'.format(embedding_type, prep_format)\n","  with open('./dataset_files/Encoded/{}/{}'.format(encoding_format,file_name), 'rb') as filehandle:\n","    encoded_test_data = pickle.load(filehandle)\n","\n","  # LABELS\n","  test_dataset_df = pd.read_pickle('./dataset_files/preprocessed_test_dataset.data', None)\n","  test_labels = test_dataset_df.loc[:,labels_to_return]\n","\n","  print('\\nProcess complete')\n","  print('{} test instances retrieved'.format(len(encoded_test_data)))\n","\n","  # Check encodings dimensions\n","  encodings_dim = encoded_test_data.shape\n","\n","  print('\\nencodings_dim = {}'.format(encodings_dim[1:]))\n","\n","  return(encoded_test_data, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_9SoIfqGUvW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616486650641,"user_tz":360,"elapsed":1176,"user":{"displayName":"Felipe RB","photoUrl":"","userId":"00787313856272916889"}},"outputId":"cb5bab1c-5370-4d4e-ac87-274d5e6d6df3"},"source":["X_test, Y_test = loadEncodedTestData(embedding_type='FT3',\n","                                    encoding_format='SINGLE-VEC',\n","                                    labels_to_return=['HS'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["FastText 3 - Spanish Unannotated Corpora\n","Encoding Format: SINGLE-VEC\n","\n","Process complete\n","1600 test instances retrieved\n","\n","encodings_dim = (300,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"js-Q7-UqHa4f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616486717855,"user_tz":360,"elapsed":486,"user":{"displayName":"Felipe RB","photoUrl":"","userId":"00787313856272916889"}},"outputId":"f34aa039-7df6-4a86-e9f2-0837c6ec23ab"},"source":["X_test.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1600, 300)"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"u0s3YJ0WHOnr"},"source":["target = Y_test['HS']\n","predicted  = np.where(my_keras_model.predict(X_test) > 0.5, 1, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8bhQy-uQF8KO"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","accuracy_s = accuracy_score(target, predicted)\n","A_f1 = f1_score(target, predicted, average=\"macro\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BjZk_AFEHvlC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487338479,"user_tz":360,"elapsed":5414,"user":{"displayName":"Felipe RB","photoUrl":"","userId":"00787313856272916889"}},"outputId":"abbe32b9-1e00-45bb-9ac0-2bdd12e2b90a"},"source":["accuracy_s, A_f1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.71375, 0.6901002945062158)"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"IDJ2TNIiEjSN"},"source":["# Resources\n","\n","* Mini batch size selection: [1](https://datascience.stackexchange.com/questions/18414/are-there-any-rules-for-choosing-the-size-of-a-mini-batch), [2](https://www.quora.com/In-deep-learning-why-dont-we-use-the-whole-training-set-to-compute-the-gradient), [3](https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu)\n","* Optimizers: [1](https://ai.stackexchange.com/questions/18206/what-kind-of-optimizer-is-suggested-to-use-for-binary-classification-of-similar)\n","* RMSprop: [1](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a)\n","* Number of Hidden Layers: [1](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)\n","\n","    \n"]}]}